{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T21:17:40.292988Z",
     "start_time": "2021-07-21T21:14:45.202115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-21 21:14:45--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2021-07-21 21:14:51--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2021-07-21 21:14:52--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.19MB/s    in 2m 47s  \n",
      "\n",
      "2021-07-21 21:17:40 (4.91 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T21:18:03.872229Z",
     "start_time": "2021-07-21T21:17:40.296415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T21:18:22.547056Z",
     "start_time": "2021-07-21T21:18:03.876475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-21 21:18:03--  https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 41696084 (40M) [application/zip]\n",
      "Saving to: ‘QQP-clean.zip’\n",
      "\n",
      "QQP-clean.zip       100%[===================>]  39.76M  5.18MB/s    in 8.8s    \n",
      "\n",
      "2021-07-21 21:18:22 (4.53 MB/s) - ‘QQP-clean.zip’ saved [41696084/41696084]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T21:18:23.901719Z",
     "start_time": "2021-07-21T21:18:22.549922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  QQP-clean.zip\n",
      "   creating: QQP/\n",
      "  inflating: QQP/train.tsv           \n",
      "  inflating: QQP/dev.tsv             \n",
      "  inflating: QQP/test.tsv            \n"
     ]
    }
   ],
   "source": [
    "!unzip QQP-clean.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T21:21:23.034179Z",
     "start_time": "2021-07-21T21:21:12.755884Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/atuthvatullin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T17:15:56.028206Z",
     "start_time": "2021-07-22T17:15:56.018689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2],[2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T17:17:26.121718Z",
     "start_time": "2021-07-22T17:17:26.111649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4472, 0.8944],\n",
       "        [0.7071, 0.7071]])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.normalize(torch.FloatTensor([[1,2], [2,2]]), p=2.0, dim=1, eps=1e-12, out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T13:37:56.559841Z",
     "start_time": "2021-09-13T13:37:56.410315Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Union, Callable\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Замените пути до директорий и файлов! Можете использовать для локальной отладки. \n",
    "# При проверке на сервере пути будут изменены\n",
    "glue_qqp_dir = 'QQP/'\n",
    "glove_path = 'glove.6B.50d.txt'\n",
    "\n",
    "\n",
    "class GaussianKernel(torch.nn.Module):\n",
    "    def __init__(self, mu: float = 1., sigma: float = 1.):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_sq = (x - self.mu)**2\n",
    "        return np.exp(-norm_sq/(2*self.sigma**2))\n",
    "    \n",
    "\n",
    "class KNRM(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix: np.ndarray, freeze_embeddings: bool, kernel_num: int = 21,\n",
    "                 sigma: float = 0.1, exact_sigma: float = 0.001,\n",
    "                 out_layers: List[int] = [10, 5]):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            freeze=freeze_embeddings,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.kernel_num = kernel_num\n",
    "        self.sigma = sigma\n",
    "        self.exact_sigma = exact_sigma\n",
    "        self.out_layers = out_layers\n",
    "\n",
    "        self.kernels = self._get_kernels_layers()\n",
    "\n",
    "        self.mlp = self._get_mlp()\n",
    "\n",
    "        self.out_activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def _get_kernels_layers(self) -> torch.nn.ModuleList:\n",
    "        \n",
    "        kernels = torch.nn.ModuleList()\n",
    "        \n",
    "        # mu\n",
    "        l_mus = [1]\n",
    "        if self.kernel_num > 1:\n",
    "            bin_size = (1-(-1)) / (self.kernel_num - 1)  # score range from [-1, 1]\n",
    "            l_mus.append(1 - bin_size / 2)  # mu: middle of the bin\n",
    "            for i in range(1, self.kernel_num - 1):\n",
    "                l_mus.append(l_mus[i] - bin_size)\n",
    "            l_mus = sorted(l_mus) # сортируем по возрастанию\n",
    "        \n",
    "        # sigma\n",
    "        l_sigmas = [self.exact_sigma]\n",
    "        if self.kernel_num > 1:\n",
    "            l_sigmas = [self.sigma]*(self.kernel_num-1) + l_sigmas\n",
    "    \n",
    "        for i in range(self.kernel_num):\n",
    "            gk = GaussianKernel(mu=l_mus[i], sigma=l_sigmas[i])\n",
    "            kernels.append(gk)\n",
    "            \n",
    "        return kernels\n",
    "\n",
    "    def _get_mlp(self) -> torch.nn.Sequential:\n",
    "        \n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        \n",
    "        if len(self.out_layers) == 0:\n",
    "            self.layers.append(torch.nn.Linear(len(self.kernels), 1))\n",
    "            \n",
    "        elif len(self.out_layers) == 1:\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Linear(len(self.kernels), self.out_layers[0]))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Linear(self.out_layers[0], 1))\n",
    "            \n",
    "        else: \n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Linear(len(self.kernels), self.out_layers[0]))\n",
    "                \n",
    "            for i in range(len(self.out_layers)):\n",
    "                \n",
    "                if i!=len(self.out_layers)-1:\n",
    "                    self.layers.append(torch.nn.ReLU())\n",
    "                    self.layers.append(torch.nn.Linear(self.out_layers[i], self.out_layers[i+1]))\n",
    "\n",
    "                else: # последний слой\n",
    "                    self.layers.append(torch.nn.ReLU())\n",
    "                    self.layers.append(torch.nn.Linear(self.out_layers[-1], 1))\n",
    "            \n",
    "#             K->ReLU->10->ReLU->5->ReLU->1\n",
    "\n",
    "        return torch.nn.Sequential(*self.layers)\n",
    "\n",
    "\n",
    "    def sim_matrix(self,a, b, eps=1e-8):\n",
    "        \n",
    "#         cos = torch.nn.CosineSimilarity(dim=-1, eps=eps)\n",
    "#         a = F.normalize(a)\n",
    "#         b = F.normalize(b)\n",
    "#         sim_mt = cos(a.unsqueeze(1), b)\n",
    "        \n",
    "        a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "        a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "        b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "        sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "        return sim_mt\n",
    "\n",
    "            \n",
    "    def _get_matching_matrix(self, query: torch.Tensor, doc: torch.Tensor) -> torch.FloatTensor:\n",
    "        \n",
    "        vA = self.embeddings(query)\n",
    "        vB = self.embeddings(doc) \n",
    "        output = []\n",
    "        for i in range(len(vA)):\n",
    "            s = self.sim_matrix(vA[i], vB[i])\n",
    "            output.append(s)\n",
    "        matrix = torch.stack(output)\n",
    "        \n",
    "        return matrix        \n",
    "                                        \n",
    "    def _apply_kernels(self, matching_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        KM = []\n",
    "        for kernel in self.kernels:\n",
    "            # shape = [B]\n",
    "            K = torch.log1p(kernel(matching_matrix).sum(dim=-1)).sum(dim=-1)\n",
    "            KM.append(K)\n",
    "        # shape = [B, K]\n",
    "        kernels_out = torch.stack(KM, dim=1)\n",
    "        return kernels_out\n",
    "\n",
    "    def predict(self, inputs: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        # shape = [Batch, Left, Embedding], [Batch, Right, Embedding]\n",
    "        query, doc = inputs['query'], inputs['document']\n",
    "        # shape = [Batch, Left, Right]\n",
    "        matching_matrix = self._get_matching_matrix(query, doc)\n",
    "        # shape = [Batch, Kernels]\n",
    "        kernels_out = self._apply_kernels(matching_matrix)\n",
    "        # shape = [Batch]\n",
    "        out = self.mlp(kernels_out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, input_1: Dict[str, torch.Tensor], input_2: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        logits_1 = self.predict(input_1)\n",
    "        logits_2 = self.predict(input_2)\n",
    "\n",
    "        logits_diff = logits_1 - logits_2\n",
    "\n",
    "        out = self.out_activation(logits_diff)\n",
    "        return out\n",
    "    \n",
    "class RankingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, index_pairs_or_triplets: List[List[Union[str, float]]],\n",
    "                 idx_to_text_mapping: Dict[str, str], vocab: Dict[str, int], oov_val: int,\n",
    "                 preproc_func: Callable, max_len: int = 30):\n",
    "        self.index_pairs_or_triplets = index_pairs_or_triplets\n",
    "        self.idx_to_text_mapping = idx_to_text_mapping\n",
    "        self.vocab = vocab\n",
    "        self.oov_val = oov_val\n",
    "        self.preproc_func = preproc_func\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_pairs_or_triplets)\n",
    "\n",
    "    def _tokenized_text_to_index(self, tokenized_text: List[str]) -> List[int]:\n",
    "        return [self.vocab.get(i, self.oov_val) for i in tokenized_text]\n",
    "\n",
    "    def _convert_text_idx_to_token_idxs(self, idx: int) -> List[int]:\n",
    "        text = self.idx_to_text_mapping[idx]\n",
    "        tokenized_text = self.preproc_func(text)\n",
    "        token_idx = self._tokenized_text_to_index(tokenized_text)\n",
    "        return token_idx\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TrainTripletsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        triplet = self.index_pairs_or_triplets[idx]\n",
    "                \n",
    "        left_elem = {'query': self._convert_text_idx_to_token_idxs(triplet[0]),\n",
    "                    'document': self._convert_text_idx_to_token_idxs(triplet[1])}\n",
    "        \n",
    "        right_elem = {'query': self._convert_text_idx_to_token_idxs(triplet[0]),\n",
    "                    'document': self._convert_text_idx_to_token_idxs(triplet[2])}\n",
    "        \n",
    "        label = triplet[-1]\n",
    "        \n",
    "        return left_elem, right_elem, label\n",
    "\n",
    "\n",
    "class ValPairsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        duplet = self.index_pairs_or_triplets[idx]\n",
    "                \n",
    "        left_elem = {'query': self._convert_text_idx_to_token_idxs(duplet[0]),\n",
    "                    'document': self._convert_text_idx_to_token_idxs(duplet[1])}\n",
    "\n",
    "        label = duplet[-1]\n",
    "        \n",
    "        return left_elem, label\n",
    "\n",
    "\n",
    "def collate_fn(batch_objs: List[Union[Dict[str, torch.Tensor], torch.FloatTensor]]):\n",
    "    max_len_q1 = -1\n",
    "    max_len_d1 = -1\n",
    "    max_len_q2 = -1\n",
    "    max_len_d2 = -1\n",
    "\n",
    "    is_triplets = False\n",
    "    for elem in batch_objs:\n",
    "        if len(elem) == 3:\n",
    "            left_elem, right_elem, label = elem\n",
    "            is_triplets = True\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        max_len_q1 = max(len(left_elem['query']), max_len_q1)\n",
    "        max_len_d1 = max(len(left_elem['document']), max_len_d1)\n",
    "        if len(elem) == 3:\n",
    "            max_len_q2 = max(len(right_elem['query']), max_len_q2)\n",
    "            max_len_d2 = max(len(right_elem['document']), max_len_d2)\n",
    "\n",
    "    q1s = []\n",
    "    d1s = []\n",
    "    q2s = []\n",
    "    d2s = []\n",
    "    labels = []\n",
    "\n",
    "    for elem in batch_objs:\n",
    "        if is_triplets:\n",
    "            left_elem, right_elem, label = elem\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        pad_len1 = max_len_q1 - len(left_elem['query'])\n",
    "        pad_len2 = max_len_d1 - len(left_elem['document'])\n",
    "        if is_triplets:\n",
    "            pad_len3 = max_len_q2 - len(right_elem['query'])\n",
    "            pad_len4 = max_len_d2 - len(right_elem['document'])\n",
    "\n",
    "        q1s.append(left_elem['query'] + [0] * pad_len1)\n",
    "        d1s.append(left_elem['document'] + [0] * pad_len2)\n",
    "        if is_triplets:\n",
    "            q2s.append(right_elem['query'] + [0] * pad_len3)\n",
    "            d2s.append(right_elem['document'] + [0] * pad_len4)\n",
    "        labels.append([label])\n",
    "    q1s = torch.LongTensor(q1s)\n",
    "    d1s = torch.LongTensor(d1s)\n",
    "    if is_triplets:\n",
    "        q2s = torch.LongTensor(q2s)\n",
    "        d2s = torch.LongTensor(d2s)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "\n",
    "    ret_left = {'query': q1s, 'document': d1s}\n",
    "    if is_triplets:\n",
    "        ret_right = {'query': q2s, 'document': d2s}\n",
    "        return ret_left, ret_right, labels\n",
    "    else:\n",
    "        return ret_left, labels\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def __init__(self, glue_qqp_dir: str, glove_vectors_path: str,\n",
    "                 min_token_occurancies: int = 1,\n",
    "                 random_seed: int = 0,\n",
    "                 emb_rand_uni_bound: float = 0.2,\n",
    "                 freeze_knrm_embeddings: bool = True,\n",
    "                 knrm_kernel_num: int = 21,\n",
    "                 knrm_out_mlp: List[int] = [],\n",
    "                 dataloader_bs: int = 1024,\n",
    "                 train_lr: float = 0.01, # 0.001\n",
    "                 change_train_loader_ep: int = 10\n",
    "                 ):\n",
    "        self.glue_qqp_dir = glue_qqp_dir\n",
    "        self.glove_vectors_path = glove_vectors_path\n",
    "        \n",
    "        self.glue_train_df = self.get_glue_df('train')\n",
    "        self.glue_dev_df = self.get_glue_df('dev')\n",
    "        self.dev_pairs_for_ndcg = self.create_val_pairs(self.glue_dev_df)\n",
    "        self.min_token_occurancies = min_token_occurancies\n",
    "        self.all_tokens = self.get_all_tokens(\n",
    "            [self.glue_train_df, self.glue_dev_df], self.min_token_occurancies)\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        self.emb_rand_uni_bound = emb_rand_uni_bound\n",
    "        self.freeze_knrm_embeddings = freeze_knrm_embeddings\n",
    "        self.knrm_kernel_num = knrm_kernel_num\n",
    "        self.knrm_out_mlp = knrm_out_mlp\n",
    "        self.dataloader_bs = dataloader_bs\n",
    "        self.train_lr = train_lr\n",
    "        self.change_train_loader_ep = change_train_loader_ep\n",
    "\n",
    "        self.model, self.vocab, self.unk_words = self.build_knrm_model()\n",
    "        \n",
    "        self.idx_to_text_mapping_train = self.get_idx_to_text_mapping(\n",
    "            self.glue_train_df)\n",
    "        self.idx_to_text_mapping_dev = self.get_idx_to_text_mapping(\n",
    "            self.glue_dev_df)\n",
    "        \n",
    "        self.val_dataset = ValPairsDataset(self.dev_pairs_for_ndcg, \n",
    "              self.idx_to_text_mapping_dev, \n",
    "              vocab=self.vocab, oov_val=self.vocab['OOV'], \n",
    "              preproc_func=self.simple_preproc)\n",
    "        self.val_dataloader = torch.utils.data.DataLoader(\n",
    "            self.val_dataset, batch_size=self.dataloader_bs, num_workers=0, \n",
    "            collate_fn=collate_fn, shuffle=False)\n",
    "        \n",
    "        \n",
    "        self.train_triplets_for_ndcg = self.sample_data_for_train_iter(self.glue_train_df, seed=42)\n",
    "        \n",
    "        self.train_dataset = TrainTripletsDataset(self.train_triplets_for_ndcg, \n",
    "              self.idx_to_text_mapping_train, \n",
    "              vocab=self.vocab, oov_val=self.vocab['OOV'], \n",
    "              preproc_func=self.simple_preproc)\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset, batch_size=self.dataloader_bs, num_workers=0, \n",
    "            collate_fn=collate_fn, shuffle=False)\n",
    "        \n",
    "\n",
    "    def get_glue_df(self, partition_type: str) -> pd.DataFrame:\n",
    "        assert partition_type in ['dev', 'train']\n",
    "        glue_df = pd.read_csv(\n",
    "            self.glue_qqp_dir + f'/{partition_type}.tsv', sep='\\t', error_bad_lines=False, dtype=object)\n",
    "        glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "        glue_df_fin = pd.DataFrame({\n",
    "            'id_left': glue_df['qid1'],\n",
    "            'id_right': glue_df['qid2'],\n",
    "            'text_left': glue_df['question1'],\n",
    "            'text_right': glue_df['question2'],\n",
    "            'label': glue_df['is_duplicate'].astype(int)\n",
    "        })\n",
    "        return glue_df_fin\n",
    "\n",
    "    def hadle_punctuation(self, inp_str: str) -> str:\n",
    "        trans_table={key:' ' for key in string.punctuation}\n",
    "        out_str = inp_str.translate(str.maketrans(trans_table))\n",
    "        return out_str\n",
    "\n",
    "    def simple_preproc(self, inp_str: str) -> List[str]:\n",
    "\n",
    "        out_str = self.hadle_punctuation(inp_str)\n",
    "        out_str = out_str.lower()\n",
    "        out_list = nltk.word_tokenize(out_str)\n",
    "        return out_list\n",
    "    \n",
    "    def _filter_rare_words(self, vocab: Dict[str, int], min_occurancies: int) -> Dict[str, int]:\n",
    "        vocab = {k: v for k, v in vocab.items() if v >= min_occurancies}\n",
    "        return vocab\n",
    "    \n",
    "    def get_all_tokens(self, list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:\n",
    "        texts_list = []\n",
    "        for df in list_of_df:\n",
    "            texts_list.append(df['text_left'].tolist())\n",
    "            texts_list.append(df['text_right'].tolist())\n",
    "            \n",
    "        texts_list = [i for sl in texts_list for i in sl]\n",
    "        \n",
    "        texts_list = list(set(texts_list))\n",
    "        \n",
    "        texts_list = [self.simple_preproc(t) for t in texts_list]\n",
    "        \n",
    "        texts_list = [i for sl in texts_list for i in sl]\n",
    "                \n",
    "        tokens_dict = dict(Counter(texts_list))\n",
    "        tokens_dict = self._filter_rare_words(tokens_dict, min_occurancies)\n",
    "        all_tokens = list(tokens_dict.keys())\n",
    "        \n",
    "        return all_tokens\n",
    "        \n",
    "    def _read_glove_embeddings(self, file_path: str) -> Dict[str, List[str]]:\n",
    "        embed_index = {}\n",
    "        f = open(file_path, encoding='utf-8')\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embed_index[word] = coefs\n",
    "        f.close()\n",
    "        return embed_index\n",
    "    \n",
    "    def create_glove_emb_from_file(self, file_path: str, inner_keys: List[str],\n",
    "                                   random_seed: int, rand_uni_bound: float\n",
    "                                   ) -> Tuple[np.ndarray, Dict[str, int], List[str]]:\n",
    "        \n",
    "        np.random.seed(random_seed)\n",
    "        emb_rand = np.random.uniform(-rand_uni_bound, rand_uni_bound, 50)\n",
    "\n",
    "        unk_words = [\"PAD\", \"OOV\"]\n",
    "        emb_matrix = np.tile(emb_rand, (len(inner_keys)+2, 1))\n",
    "        emb_matrix[0] = np.zeros(50)\n",
    "        \n",
    "        embed_index = self._read_glove_embeddings(file_path)\n",
    "        \n",
    "        for i, word in enumerate(inner_keys):\n",
    "            word_emb = embed_index.get(word)\n",
    "            if word_emb is not None:\n",
    "                emb_matrix[i+2] = word_emb\n",
    "            else:\n",
    "                unk_words.append(word)\n",
    "           \n",
    "        vocab = dict(zip(inner_keys, range(2,len(inner_keys)+2)))\n",
    "        \n",
    "        vocab.update({\n",
    "            \"PAD\": 0,\n",
    "            \"OOV\": 1\n",
    "        })   \n",
    "        \n",
    "        self.emb_matrix = emb_matrix\n",
    "        self.vocab = vocab\n",
    "        self.unk_words = unk_words\n",
    "        \n",
    "        return emb_matrix, vocab, unk_words\n",
    "\n",
    "    def build_knrm_model(self) -> Tuple[torch.nn.Module, Dict[str, int], List[str]]:\n",
    "        emb_matrix, vocab, unk_words = self.create_glove_emb_from_file(\n",
    "            self.glove_vectors_path, self.all_tokens, self.random_seed, self.emb_rand_uni_bound)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        knrm = KNRM(emb_matrix, freeze_embeddings=self.freeze_knrm_embeddings,\n",
    "                    out_layers=self.knrm_out_mlp, kernel_num=self.knrm_kernel_num)\n",
    "        return knrm, vocab, unk_words\n",
    "\n",
    "\n",
    "    def sample_data_for_train_iter(self, inp_df: pd.DataFrame, seed: int\n",
    "                                   ) -> List[List[Union[str, float]]]:\n",
    "        \n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "        \n",
    "        t = inp_df_select.merge(inp_df_select, how='left', on='id_left')\n",
    "        tt = t[(t.label_x==1)&(t.label_y==0)]\n",
    "        tt['ones'] = 1\n",
    "        ttt = tt[['id_left', 'id_right_x', 'id_right_y', 'ones']].values\n",
    "\n",
    "        inp_df_select_unr = inp_df_select.copy()\n",
    "        inp_df_select_unr.loc[:,'id_right'] = inp_df_select_unr['id_right'].sample(frac=1, random_state=42).values\n",
    "\n",
    "        unr = inp_df_select_unr.merge(inp_df_select, on=['id_left', 'id_right'], how='left')\n",
    "        unr = unr[unr.label_y.isnull()]\n",
    "\n",
    "        ll = inp_df_select.merge(unr, how='left', on='id_left')\n",
    "        ll['ones'] = 1\n",
    "        lll = ll[['id_left', 'id_right_x', 'id_right_y', 'ones']].values\n",
    "\n",
    "        itog = np.concatenate((ttt,lll))\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        # np.random.shuffle(itog)\n",
    "        itog_ones_ind = np.random.choice(len(itog), size=4500, replace=False)\n",
    "        itog_ones = itog[itog_ones_ind]\n",
    "        itog_ones = itog_ones.tolist()\n",
    "\n",
    "        itog_zeros = []\n",
    "        itog_zeros_ind = np.random.choice(len(itog), size=3500, replace=False)\n",
    "        for i in itog_zeros_ind:\n",
    "            tl = itog[i]\n",
    "            q1 = tl[0]\n",
    "            q2 = tl[2]\n",
    "            q3 = tl[1]\n",
    "            itog_zeros.append([q1, q2, q3, 0])\n",
    "\n",
    "        final = itog_ones + itog_zeros\n",
    "        np.random.shuffle(final)\n",
    "\n",
    "        return final\n",
    "\n",
    "    def create_val_pairs(self, inp_df: pd.DataFrame, fill_top_to: int = 15,\n",
    "                         min_group_size: int = 2, seed: int = 0) -> List[List[Union[str, float]]]:\n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "        inf_df_group_sizes = inp_df_select.groupby('id_left').size()\n",
    "        glue_dev_leftids_to_use = list(\n",
    "            inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
    "        groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "            glue_dev_leftids_to_use)].groupby('id_left')\n",
    "\n",
    "        all_ids = set(inp_df['id_left']).union(set(inp_df['id_right']))\n",
    "\n",
    "        out_pairs = []\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        for id_left, group in groups:\n",
    "            ones_ids = group[group.label > 0].id_right.values\n",
    "            zeroes_ids = group[group.label == 0].id_right.values\n",
    "            sum_len = len(ones_ids) + len(zeroes_ids)\n",
    "            num_pad_items = max(0, fill_top_to - sum_len)\n",
    "            if num_pad_items > 0:\n",
    "                cur_chosen = set(ones_ids).union(\n",
    "                    set(zeroes_ids)).union({id_left})\n",
    "                pad_sample = np.random.choice(\n",
    "                    list(all_ids - cur_chosen), num_pad_items, replace=False).tolist()\n",
    "            else:\n",
    "                pad_sample = []\n",
    "            for i in ones_ids:\n",
    "                out_pairs.append([id_left, i, 2])\n",
    "            for i in zeroes_ids:\n",
    "                out_pairs.append([id_left, i, 1])\n",
    "            for i in pad_sample:\n",
    "                out_pairs.append([id_left, i, 0])\n",
    "        return out_pairs\n",
    "\n",
    "    def get_idx_to_text_mapping(self, inp_df: pd.DataFrame) -> Dict[str, str]:\n",
    "        left_dict = (\n",
    "            inp_df\n",
    "            [['id_left', 'text_left']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_left')\n",
    "            ['text_left']\n",
    "            .to_dict()\n",
    "        )\n",
    "        right_dict = (\n",
    "            inp_df\n",
    "            [['id_right', 'text_right']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_right')\n",
    "            ['text_right']\n",
    "            .to_dict()\n",
    "        )\n",
    "        left_dict.update(right_dict)\n",
    "        return left_dict\n",
    "\n",
    "    def ndcg_k(self, ys_true: np.array, ys_pred: np.array, ndcg_top_k: int = 10) -> float:\n",
    "        \n",
    "        ys_ideal_sort_index = np.argsort(ys_true)[::-1]\n",
    "        ys_ideal_true_sort = ys_true[ys_ideal_sort_index]\n",
    "    \n",
    "        if ndcg_top_k:\n",
    "            ys_ideal_true_sort = ys_ideal_true_sort[:ndcg_top_k]\n",
    "\n",
    "        ideal_dcg = 0\n",
    "        for ix, e in enumerate(ys_ideal_true_sort):\n",
    "            ideal_dcg += self.compute_gain(float(e), gain_scheme=\"exp2\")/math.log2(ix+2)\n",
    "    \n",
    "        dcg_ = self.dcg(ys_true, ys_pred, gain_scheme=\"exp2\", top_k=ndcg_top_k)\n",
    "                \n",
    "        return dcg_/ideal_dcg\n",
    "    \n",
    "    def compute_gain(self, y_value: float, gain_scheme: str) -> float:\n",
    "        if gain_scheme==\"const\":\n",
    "            return y_value\n",
    "        elif gain_scheme==\"exp2\":\n",
    "            # return 2**y_value - 1\n",
    "            return pow(2, y_value) - 1\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def dcg(self, ys_true: np.array, ys_pred: np.array, gain_scheme: str, top_k: int) -> float:\n",
    "        \n",
    "        sort_indices = np.argsort(ys_pred)[::-1]\n",
    "        ys_true_sort = ys_true[sort_indices]\n",
    "                \n",
    "        if top_k:\n",
    "            ys_true_sort = ys_true_sort[:top_k]\n",
    "            \n",
    "        dcg_ = 0\n",
    "        for ix, e in enumerate(ys_true_sort):\n",
    "            dcg_ += self.compute_gain(float(e), gain_scheme)/math.log2(ix+2)\n",
    "\n",
    "        return dcg_\n",
    "    \n",
    "    def valid(self, model: torch.nn.Module, val_dataloader: torch.utils.data.DataLoader) -> float:\n",
    "        labels_and_groups = val_dataloader.dataset.index_pairs_or_triplets\n",
    "        labels_and_groups = pd.DataFrame(labels_and_groups, columns=['left_id', 'right_id', 'rel'])\n",
    "        \n",
    "        all_preds = []\n",
    "        for batch in (val_dataloader):\n",
    "            inp_1, y = batch\n",
    "            preds = model.predict(inp_1)\n",
    "            preds_np = preds.detach().numpy()\n",
    "            all_preds.append(preds_np)\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        labels_and_groups['preds'] = all_preds\n",
    "        \n",
    "        ndcgs = []\n",
    "        for cur_id in labels_and_groups.left_id.unique():\n",
    "            cur_df = labels_and_groups[labels_and_groups.left_id == cur_id]\n",
    "            ndcg = self.ndcg_k(cur_df.rel.values.reshape(-1), cur_df.preds.values.reshape(-1))\n",
    "            if np.isnan(ndcg):\n",
    "                ndcgs.append(0)\n",
    "            else:\n",
    "                ndcgs.append(ndcg)\n",
    "        return np.mean(ndcgs)\n",
    "\n",
    "    def train(self, n_epochs: int):\n",
    "        \n",
    "        opt = torch.optim.SGD(self.model.parameters(), lr=self.train_lr)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for e in range(n_epochs):\n",
    "            \n",
    "            loss = 0\n",
    "            \n",
    "#             if (e%self.change_train_loader_ep == 0) & (e > 0):\n",
    "                \n",
    "#                 self.train_triplets_for_ndcg = self.sample_data_for_train_iter(self.glue_train_df, seed=42+e)\n",
    "\n",
    "#                 self.train_dataset = TrainTripletsDataset(self.train_triplets_for_ndcg, \n",
    "#                       self.idx_to_text_mapping_train, \n",
    "#                       vocab=self.vocab, oov_val=self.vocab['OOV'], \n",
    "#                       preproc_func=self.simple_preproc)\n",
    "#                 self.train_dataloader = torch.utils.data.DataLoader(\n",
    "#                     self.train_dataset, batch_size=self.dataloader_bs, num_workers=0, \n",
    "#                     collate_fn=collate_fn, shuffle=False)\n",
    "            \n",
    "            for train_batch in self.train_dataloader:\n",
    "                opt.zero_grad()\n",
    "                inp_1, inp_2, batch_ys = train_batch\n",
    "                batch_pred = self.model(inp_1, inp_2)\n",
    "                batch_loss = criterion(batch_pred, batch_ys)\n",
    "                batch_loss.backward()\n",
    "                opt.step()\n",
    "                loss += batch_loss.item()\n",
    "                \n",
    "            val_ndcg = self.valid(self.model, self.val_dataloader)\n",
    "            print(f\"Epoch {e} | BCELoss {np.mean(loss)} | NDCG@k {val_ndcg}\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T13:37:42.287267Z",
     "start_time": "2021-09-13T13:37:42.166873Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d617b4cef7aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_triplets_for_ndcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_data_for_train_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglue_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "s.train_triplets_for_ndcg = s.sample_data_for_train_iter(s.glue_train_df, seed=42+e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T17:22:14.596608Z",
     "start_time": "2021-07-23T17:22:14.589504Z"
    }
   },
   "outputs": [],
   "source": [
    "s.train_dataset = TrainTripletsDataset(s.train_triplets_for_ndcg, \n",
    "  s.idx_to_text_mapping_train, \n",
    "  vocab=s.vocab, oov_val=s.vocab['OOV'], \n",
    "  preproc_func=s.simple_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T17:22:17.849588Z",
     "start_time": "2021-07-23T17:22:17.842411Z"
    }
   },
   "outputs": [],
   "source": [
    "s.train_dataloader = torch.utils.data.DataLoader(\n",
    "s.train_dataset, batch_size=s.dataloader_bs, num_workers=0, \n",
    "collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T17:23:04.509009Z",
     "start_time": "2021-07-23T17:23:01.154964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'query': tensor([[  13,   22,   21,  ...,    0,    0,    0],\n",
      "        [  80, 7450,   15,  ...,    0,    0,    0],\n",
      "        [  26,   22,   23,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  26,    2,   27,  ...,    0,    0,    0],\n",
      "        [  26,  168,   27,  ...,    0,    0,    0],\n",
      "        [  72, 2877,  541,  ...,    0,    0,    0]]), 'document': tensor([[   13,    75,    21,  ...,     0,     0,     0],\n",
      "        [   13,    80,  1382,  ...,     0,     0,     0],\n",
      "        [   13,   132, 11724,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   26,     2,    27,  ...,     0,     0,     0],\n",
      "        [   26,   158,   205,  ...,     0,     0,     0],\n",
      "        [   26,   168,    27,  ...,     0,     0,     0]])}, {'query': tensor([[  13,   22,   21,  ...,    0,    0,    0],\n",
      "        [  80, 7450,   15,  ...,    0,    0,    0],\n",
      "        [  26,   22,   23,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  26,    2,   27,  ...,    0,    0,    0],\n",
      "        [  26,  168,   27,  ...,    0,    0,    0],\n",
      "        [  72, 2877,  541,  ...,    0,    0,    0]]), 'document': tensor([[   23,   192,    15,  ...,     0,     0,     0],\n",
      "        [   26,  3574,    80,  ...,     0,     0,     0],\n",
      "        [    2,  2573, 35702,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   26,    30,    15,  ...,     0,     0,     0],\n",
      "        [    2,     3,   139,  ...,     0,     0,     0],\n",
      "        [   26,   168,   729,  ...,     0,     0,     0]])}, tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        ...,\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]))\n",
      "({'query': tensor([[  26,    2,   27,  ...,    0,    0,    0],\n",
      "        [  80, 9713,   84,  ...,    0,    0,    0],\n",
      "        [  37,   80, 1922,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  13,   14,   23,  ...,    0,    0,    0],\n",
      "        [  13,  132,   23,  ...,    0,    0,    0],\n",
      "        [  26,    2,   27,  ...,    0,    0,    0]]), 'document': tensor([[   22, 14109,  3174,  ...,     0,     0,     0],\n",
      "        [   23,   910,   357,  ...,     0,     0,     0],\n",
      "        [   13,    80,  1337,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   26,   168,   139,  ...,     0,     0,     0],\n",
      "        [   26,     2,    27,  ...,     0,     0,     0],\n",
      "        [   26,   168,   139,  ...,     0,     0,     0]])}, {'query': tensor([[  26,    2,   27,  ...,    0,    0,    0],\n",
      "        [  80, 9713,   84,  ...,    0,    0,    0],\n",
      "        [  37,   80, 1922,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  13,   14,   23,  ...,    0,    0,    0],\n",
      "        [  13,  132,   23,  ...,    0,    0,    0],\n",
      "        [  26,    2,   27,  ...,    0,    0,    0]]), 'document': tensor([[  22,    3,  205,  ...,    0,    0,    0],\n",
      "        [ 132, 9713,  178,  ...,    0,    0,    0],\n",
      "        [  37,   80, 1922,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,   81,   49,  ...,    0,    0,    0],\n",
      "        [  26,  168,  139,  ...,    0,    0,    0],\n",
      "        [  26,   22,   23,  ...,    0,    0,    0]])}, tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        ...,\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]))\n",
      "({'query': tensor([[  13,  604,   23,  ...,    0,    0,    0],\n",
      "        [  26, 3000, 5380,  ...,    0,    0,    0],\n",
      "        [  26,    2,   27,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  13,  132,   23,  ...,    0,    0,    0],\n",
      "        [  26,  168,   27,  ...,    0,    0,    0],\n",
      "        [  13,   22,   23,  ...,    0,    0,    0]]), 'document': tensor([[  13,   71, 7471,  ...,    0,    0,    0],\n",
      "        [  26,    2,   27,  ...,    0,    0,    0],\n",
      "        [  26, 2280, 1042,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  13,   80,   59,  ...,    0,    0,    0],\n",
      "        [  13,  132,  413,  ...,    0,    0,    0],\n",
      "        [  13,   22,   23,  ...,    0,    0,    0]])}, {'query': tensor([[  13,  604,   23,  ...,    0,    0,    0],\n",
      "        [  26, 3000, 5380,  ...,    0,    0,    0],\n",
      "        [  26,    2,   27,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  13,  132,   23,  ...,    0,    0,    0],\n",
      "        [  26,  168,   27,  ...,    0,    0,    0],\n",
      "        [  13,   22,   23,  ...,    0,    0,    0]]), 'document': tensor([[  13,  132,   23,  ...,    0,    0,    0],\n",
      "        [  26, 3000, 5380,  ...,    0,    0,    0],\n",
      "        [  23,   75, 8736,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  26,    2,   27,  ...,    0,    0,    0],\n",
      "        [  26,  168,  139,  ...,    0,    0,    0],\n",
      "        [   2,    3,   15,  ...,    0,    0,    0]])}, tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        ...,\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]))\n",
      "({'query': tensor([[  13,   14,   23,  ...,    0,    0,    0],\n",
      "        [  58, 2046, 2900,  ...,    0,    0,    0],\n",
      "        [  26,  305,   48,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  26,  158,  231,  ...,    0,    0,    0],\n",
      "        [   2,  857, 3223,  ...,    0,    0,    0],\n",
      "        [  26,   30,   27,  ...,    0,    0,    0]]), 'document': tensor([[  13,  132,   23,  ...,    0,    0,    0],\n",
      "        [  58, 2046, 2900,  ...,    0,    0,    0],\n",
      "        [  26,   71,   27,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  26,   30,   15,  ...,    0,    0,    0],\n",
      "        [   2,   99, 4438,  ...,    0,    0,    0],\n",
      "        [  13,  132,   23,  ...,    0,    0,    0]])}, {'query': tensor([[  13,   14,   23,  ...,    0,    0,    0],\n",
      "        [  58, 2046, 2900,  ...,    0,    0,    0],\n",
      "        [  26,  305,   48,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  26,  158,  231,  ...,    0,    0,    0],\n",
      "        [   2,  857, 3223,  ...,    0,    0,    0],\n",
      "        [  26,   30,   27,  ...,    0,    0,    0]]), 'document': tensor([[   48,  3071,  1951,  ...,     0,     0,     0],\n",
      "        [  151,    26,   153,  ...,     0,     0,     0],\n",
      "        [  229,   785,   982,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   26,     2,   128,  ...,     0,     0,     0],\n",
      "        [   26,   132,    21,  ...,     0,     0,     0],\n",
      "        [    2,  4145, 20904,  ...,     0,     0,     0]])}, tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]))\n",
      "({'query': tensor([[   26,   168,    27,  ...,     0,     0,     0],\n",
      "        [   14,    23,  5016,  ...,     0,     0,     0],\n",
      "        [   58, 37502,     2,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   26,    71, 10965,  ...,     0,     0,     0],\n",
      "        [   13,   964,  1694,  ...,     0,     0,     0],\n",
      "        [   13,   132,  1120,  ...,     0,     0,     0]]), 'document': tensor([[   26,    30,    27,  ...,     0,     0,     0],\n",
      "        [   13,   132,   413,  ...,     0,     0,     0],\n",
      "        [   26,  7660, 37502,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   26,     2,    27,  ...,     0,     0,     0],\n",
      "        [   26,    30,  8333,  ...,     0,     0,     0],\n",
      "        [   13,    14,    23,  ...,     0,     0,     0]])}, {'query': tensor([[   26,   168,    27,  ...,     0,     0,     0],\n",
      "        [   14,    23,  5016,  ...,     0,     0,     0],\n",
      "        [   58, 37502,     2,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   26,    71, 10965,  ...,     0,     0,     0],\n",
      "        [   13,   964,  1694,  ...,     0,     0,     0],\n",
      "        [   13,   132,  1120,  ...,     0,     0,     0]]), 'document': tensor([[   26,   168,    27,  ...,     0,     0,     0],\n",
      "        [   13,    22,    23,  ...,     0,     0,     0],\n",
      "        [  126,    71,  4112,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   26,    80, 10965,  ...,     0,     0,     0],\n",
      "        [  132,  6127,  7527,  ...,     0,     0,     0],\n",
      "        [   15,    52,   597,  ...,     0,     0,     0]])}, tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]))\n",
      "({'query': tensor([[   26,   229,   205,  ...,     0,     0,     0],\n",
      "        [   37,     2,  1333,  ...,     0,     0,     0],\n",
      "        [   38,    27, 15303,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   26,     2,    27,  ...,     0,     0,     0],\n",
      "        [   13,   132,    23,  ...,     0,     0,     0],\n",
      "        [    2,    99,   657,  ...,     0,     0,     0]]), 'document': tensor([[  26,    2,   27,  ...,    0,    0,    0],\n",
      "        [  26, 1450,   62,  ...,    0,    0,    0],\n",
      "        [  26,  168,   27,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  26,  168,   27,  ...,    0,    0,    0],\n",
      "        [  37,  132,  139,  ...,    0,    0,    0],\n",
      "        [   2,   99,  657,  ...,    0,    0,    0]])}, {'query': tensor([[   26,   229,   205,  ...,     0,     0,     0],\n",
      "        [   37,     2,  1333,  ...,     0,     0,     0],\n",
      "        [   38,    27, 15303,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   26,     2,    27,  ...,     0,     0,     0],\n",
      "        [   13,   132,    23,  ...,     0,     0,     0],\n",
      "        [    2,    99,   657,  ...,     0,     0,     0]]), 'document': tensor([[  26,  168,   27,  ...,    0,    0,    0],\n",
      "        [  37,    2, 1333,  ...,    0,    0,    0],\n",
      "        [  26,  246,   27,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  13,   22,   23,  ...,    0,    0,    0],\n",
      "        [  13,  132,   23,  ...,    0,    0,    0],\n",
      "        [ 120,   80,   39,  ...,    0,    0,    0]])}, tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'query': tensor([[  26,  168,  509,  ...,    0,    0,    0],\n",
      "        [  13,  132,   23,  ...,    0,    0,    0],\n",
      "        [  13,  132,   23,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  13,  132,   21,  ...,    0,    0,    0],\n",
      "        [  13,   80, 7703,  ...,    0,    0,    0],\n",
      "        [  13,  132,  137,  ...,    0,    0,    0]]), 'document': tensor([[4071, 1618,   44,  ...,    0,    0,    0],\n",
      "        [  13,  132,   23,  ...,    0,    0,    0],\n",
      "        [  13,  132,   21,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  26, 3000, 5380,  ...,    0,    0,    0],\n",
      "        [  13,   22,   23,  ...,    0,    0,    0],\n",
      "        [  13,   22,   23,  ...,    0,    0,    0]])}, {'query': tensor([[  26,  168,  509,  ...,    0,    0,    0],\n",
      "        [  13,  132,   23,  ...,    0,    0,    0],\n",
      "        [  13,  132,   23,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  13,  132,   21,  ...,    0,    0,    0],\n",
      "        [  13,   80, 7703,  ...,    0,    0,    0],\n",
      "        [  13,  132,  137,  ...,    0,    0,    0]]), 'document': tensor([[  26,  168,   27,  ...,    0,    0,    0],\n",
      "        [  22,   27,   54,  ...,    0,    0,    0],\n",
      "        [  13,  132,   23,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  13, 1568,   23,  ...,    0,    0,    0],\n",
      "        [  13,  132,   27,  ...,    0,    0,    0],\n",
      "        [  13,   22,   23,  ...,    0,    0,    0]])}, tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        ...,\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]))\n",
      "({'query': tensor([[   13,    22,    23,  ...,     0,     0,     0],\n",
      "        [   22, 29611,   479,  ...,     0,     0,     0],\n",
      "        [   26,   168,   139,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   26,   168,    27,  ...,     0,     0,     0],\n",
      "        [   26,   242,   124,  ...,     0,     0,     0],\n",
      "        [  158,    15, 20383,  ...,     0,     0,     0]]), 'document': tensor([[   26,     2,    15,  ...,     0,     0,     0],\n",
      "        [   22, 29611,   479,  ...,     0,     0,     0],\n",
      "        [   26,  1412,    52,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   13,    22,    21,  ...,     0,     0,     0],\n",
      "        [   27,  1618,    71,  ...,     0,     0,     0],\n",
      "        [  168,   474,  7912,  ...,     0,     0,     0]])}, {'query': tensor([[   13,    22,    23,  ...,     0,     0,     0],\n",
      "        [   22, 29611,   479,  ...,     0,     0,     0],\n",
      "        [   26,   168,   139,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   26,   168,    27,  ...,     0,     0,     0],\n",
      "        [   26,   242,   124,  ...,     0,     0,     0],\n",
      "        [  158,    15, 20383,  ...,     0,     0,     0]]), 'document': tensor([[  13,  132,   23,  ...,    0,    0,    0],\n",
      "        [  14,   23, 1672,  ...,    0,    0,    0],\n",
      "        [  26,  168,  139,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  26,  168,   27,  ...,    0,    0,    0],\n",
      "        [ 229, 8281,  550,  ...,    0,    0,    0],\n",
      "        [  26,   80,   27,  ...,    0,    0,    0]])}, tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]]))\n"
     ]
    }
   ],
   "source": [
    "for train_batch in s.train_dataloader:\n",
    "    print(train_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T16:16:17.239640Z",
     "start_time": "2021-07-23T16:16:17.222822Z"
    }
   },
   "source": [
    "- попробовать обучиться на 1 батч и смотреть на метрику на трейне, если все хорошо, то вы должны переобучиться, если же нет, то значит где-то в логике работы сети ошибка (в целом этот трюк часто помогает разобраться в работе сети и вообще хороший совет от Андрея Карпаты)\n",
    "\n",
    "- зачем вы оставляете (retain_graph=true)? Это кушает память GPU, а нужно чтобы иметь возможность делать несколько backward‘ов после одного forward’а\n",
    "\n",
    "- если оно учится без обнуления градиентов, то значит оно учится когда они копятся, может быть вы LR поставили очень маленький?\n",
    "\n",
    "- ну-  в смысле, это же шаг оптимизации. Я его увеличил в 10 раз (1e-3 -> 1e-2). Вполне разумно что он влияет очень сильно: если он маленький, сетка учится медленно, оптимизация сходится медленно и еще может “застакаться” в плохом (высоком) локальном минимуме. Я это заметил просто по темпу уменьшения loss-функции. В нейронках это же ну чуть ли не основной параметр для тьюнинга (не считая архитектуры сети) \n",
    "\n",
    "\n",
    "\n",
    "- Да, я понимаю, просто я думал (и так было всегда), что без обновления градиентов сетка в принципе обучаться не будет, а тут наоборот получилось)\n",
    "- ну тут если делать маленькие шаги, то за 10 эпох, оно фактически сделает 1 шаг для случая с 10xLR. Поэтому то, что оно обучалось - ну не везение, конечно, но в общем случае так не будет, особенно когда эпох много больше. Бывает, что градиенты таким образом копят (gradient accumulation trick), когда памяти для большого батча не хвататет, а хотелось бы, но их все равно обнуляют раз в несколько forward-pass’ов.\n",
    "\n",
    "\n",
    "batch_loss.backward(retain_graph=True)\n",
    "зачем вы оставляете (retain_graph=true)? \n",
    "Это кушает память GPU, а нужно чтобы иметь возможность \n",
    "делать несколько backward‘ов после одного forward’а."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T14:36:56.328274Z",
     "start_time": "2021-09-13T14:35:22.624092Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atuthvatullin/environments/albert/lib/python3.6/site-packages/ipykernel_launcher.py:448: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 33s, sys: 301 ms, total: 1min 33s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s = Solution(glue_qqp_dir=glue_qqp_dir,\n",
    "             glove_vectors_path=glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T14:49:39.194522Z",
     "start_time": "2021-09-13T14:36:56.332860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | BCELoss 74.69919818639755 | NDCG@k 0.5969478484574697\n",
      "Epoch 1 | BCELoss 67.82673746347427 | NDCG@k 0.6732350804012821\n",
      "Epoch 2 | BCELoss 10.391951382160187 | NDCG@k 0.7708105699599211\n",
      "Epoch 3 | BCELoss 14.009461492300034 | NDCG@k 0.835115058585927\n",
      "Epoch 4 | BCELoss 15.78466972708702 | NDCG@k 0.8772447190421852\n",
      "Epoch 5 | BCELoss 17.472972750663757 | NDCG@k 0.8858484989145097\n",
      "Epoch 6 | BCELoss 15.360442414879799 | NDCG@k 0.9026256244789225\n",
      "Epoch 7 | BCELoss 13.435205459594727 | NDCG@k 0.9069991742313404\n",
      "Epoch 8 | BCELoss 13.780339479446411 | NDCG@k 0.9140419785934341\n",
      "Epoch 9 | BCELoss 12.085077375173569 | NDCG@k 0.9155337981537678\n",
      "Epoch 10 | BCELoss 15.31329520046711 | NDCG@k 0.9213798461911914\n",
      "Epoch 11 | BCELoss 10.350726053118706 | NDCG@k 0.9164321154732871\n",
      "Epoch 12 | BCELoss 41.90720036625862 | NDCG@k 0.9316748454446152\n",
      "Epoch 13 | BCELoss 37.52182373404503 | NDCG@k 0.9090078951925689\n",
      "Epoch 14 | BCELoss 1.6010073274374008 | NDCG@k 0.9268422850278376\n",
      "Epoch 15 | BCELoss 9.631582036614418 | NDCG@k 0.9238197713330848\n",
      "Epoch 16 | BCELoss 36.83916883170605 | NDCG@k 0.9375117017723898\n",
      "Epoch 17 | BCELoss 61.66051855683327 | NDCG@k 0.9091206747222791\n",
      "Epoch 18 | BCELoss 37.62688498198986 | NDCG@k 0.935310857269985\n"
     ]
    }
   ],
   "source": [
    "s.train(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T22:15:21.169877Z",
     "start_time": "2021-07-28T22:15:21.137061Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(s.model.embeddings.state_dict(), 'knrm_embed.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T16:22:40.498389Z",
     "start_time": "2021-08-01T16:22:40.487813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87164"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s.model.embeddings.state_dict()['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T22:18:56.092457Z",
     "start_time": "2021-07-28T22:18:56.085795Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(s.model.mlp.state_dict(), 'mlp.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T14:50:40.330629Z",
     "start_time": "2021-09-13T14:50:40.318544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.0017,  0.1168, -0.1810, -0.1642, -0.0898,  0.0357,  0.0042,  0.2626,\n",
       "                        0.1491,  0.2527,  0.1289,  0.1276, -0.0758, -0.0461, -0.0433, -0.0088,\n",
       "                        0.0063,  0.0772,  0.0238,  0.5173,  1.0399]])),\n",
       "             ('0.bias', tensor([0.1812]))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.model.mlp.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T14:49:44.854942Z",
     "start_time": "2021-09-13T14:49:44.850089Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T14:51:16.845010Z",
     "start_time": "2021-09-13T14:51:16.833546Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(OrderedDict([('0.0.weight',\n",
    "              torch.Tensor([[-0.0017,  0.1168, -0.1810, -0.1644, -0.0903,  0.0340, -0.0185,  0.1554,\n",
    "                        0.0101,  0.1091, -0.0145, -0.0103, -0.1788, -0.0660, -0.0424, -0.0073,\n",
    "                        0.0063,  0.0771,  0.0298,  0.5381,  1.0739]])),\n",
    "             ('0.0.bias', torch.Tensor([0.1812]))]), 'mlp_1.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T14:51:28.487973Z",
     "start_time": "2021-09-13T14:51:28.482449Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(OrderedDict([('0.0.weight',\n",
    "              torch.Tensor([[-0.0017,  0.1168, -0.1810, -0.1642, -0.0898,  0.0357,  0.0042,  0.2626,\n",
    "                        0.1491,  0.2527,  0.1289,  0.1276, -0.0758, -0.0461, -0.0433, -0.0088,\n",
    "                        0.0063,  0.0772,  0.0238,  0.5173,  1.0399]])),\n",
    "             ('0.0.bias', torch.Tensor([0.1812]))]), 'mlp_2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T14:49:47.081115Z",
     "start_time": "2021-09-13T14:49:47.072552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=21, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.model.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T13:54:13.914888Z",
     "start_time": "2021-09-13T13:54:13.905942Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(s.model.mlp, 'mlp_1.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T22:16:34.685009Z",
     "start_time": "2021-07-28T22:16:34.555739Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab.json', 'w') as f:\n",
    "    json.dump(s.vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T16:22:54.725893Z",
     "start_time": "2021-08-01T16:22:54.716329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87164"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T16:16:27.139976Z",
     "start_time": "2021-08-01T16:16:27.134942Z"
    }
   },
   "outputs": [],
   "source": [
    "# load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T08:48:28.526978Z",
     "start_time": "2021-07-22T08:48:28.518294Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "knrm = KNRM(s.emb_matrix, s.freeze_knrm_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T16:55:23.190009Z",
     "start_time": "2021-07-22T16:55:22.727203Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "vA = torch.LongTensor([[np.random.randint(1000) for i in range(31)] for a in range(1024)])\n",
    "vB = torch.LongTensor([[np.random.randint(1000) for i in range(62)] for a in range(1024)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T16:55:23.205742Z",
     "start_time": "2021-07-22T16:55:23.193944Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vA = knrm.embeddings(vA)\n",
    "vB = knrm.embeddings(vB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T16:55:25.202775Z",
     "start_time": "2021-07-22T16:55:25.195155Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 31, 50]), torch.Size([1024, 62, 50]))"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vA.shape,vB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T20:50:45.995765Z",
     "start_time": "2021-07-22T20:50:45.989703Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sim_matrix(a, b, eps=1e-8):\n",
    "    \"\"\"\n",
    "    added eps for numerical stability\n",
    "    \"\"\"\n",
    "#     a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "#     a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "#     b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "#     sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "    cos = torch.nn.CosineSimilarity(dim=-1, eps=eps)\n",
    "#     a = F.normalize(a)\n",
    "#     b = F.normalize(b)\n",
    "    sim_mt = cos(a.unsqueeze(1), b)\n",
    "#     sim_mt = F.cosine_similarity(a,b)\n",
    "\n",
    "    return sim_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T20:50:48.156751Z",
     "start_time": "2021-07-22T20:50:47.933988Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in range(len(vA)):\n",
    "    s = sim_matrix(vA[i], vB[i])\n",
    "    output.append(s)\n",
    "\n",
    "matrix1 = torch.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T20:50:57.388001Z",
     "start_time": "2021-07-22T20:50:57.380319Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sim_matrix(a, b, eps=1e-8):\n",
    "    \"\"\"\n",
    "    added eps for numerical stability\n",
    "    \"\"\"\n",
    "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "    a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "    b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "    sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "    return sim_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T20:51:00.241704Z",
     "start_time": "2021-07-22T20:50:59.892580Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in range(len(vA)):\n",
    "    s = sim_matrix(vA[i], vB[i])\n",
    "    output.append(s)\n",
    "\n",
    "matrix = torch.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T20:52:07.233073Z",
     "start_time": "2021-07-22T20:52:07.199327Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(matrix, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T19:08:53.771696Z",
     "start_time": "2021-07-22T19:08:53.743128Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  2.4906e-01,  6.2370e-01,  1.1181e-01,  5.6894e-01,\n",
       "         -7.3797e-02,  6.0076e-01, -2.9285e-02,  7.3454e-01,  3.3433e-01,\n",
       "          3.8078e-01,  5.0355e-01,  2.5614e-01,  4.0707e-01,  4.0707e-01,\n",
       "          5.0197e-01,  6.0729e-01,  1.3733e-01,  4.2082e-01, -1.5886e-01,\n",
       "          5.0278e-01,  7.0347e-01,  3.1847e-01,  3.8264e-01,  5.0305e-01,\n",
       "          5.8528e-01,  1.7646e-01, -1.8621e-01,  7.0743e-01,  8.5510e-01,\n",
       "         -1.1885e-01],\n",
       "        [ 2.4906e-01,  1.0000e+00,  2.9212e-02,  2.2471e-01,  2.9357e-01,\n",
       "          6.6197e-03,  5.7279e-02,  1.1422e-01,  4.7118e-02, -5.6208e-02,\n",
       "          1.1162e-01,  3.8129e-01, -6.8250e-02, -1.8178e-01, -1.8178e-01,\n",
       "         -1.7340e-02,  1.7689e-01, -2.3004e-02, -2.0182e-02,  2.5926e-01,\n",
       "         -8.6881e-02,  1.2807e-02,  2.6289e-01,  1.4439e-01,  8.7400e-02,\n",
       "          1.9350e-01, -4.3373e-02,  1.9095e-01, -1.1387e-01,  2.6632e-01,\n",
       "          2.1493e-01],\n",
       "        [ 6.2370e-01,  2.9212e-02,  1.0000e+00,  2.1255e-01,  8.4917e-01,\n",
       "         -1.5391e-01,  7.7535e-01,  5.9487e-02,  7.7200e-01,  7.1308e-01,\n",
       "          3.1576e-01,  4.2094e-01,  4.2314e-01,  4.2689e-01,  4.2689e-01,\n",
       "          6.3480e-01,  6.6732e-01,  2.2801e-01,  3.6531e-01, -2.5719e-01,\n",
       "          7.5979e-01,  7.5469e-01,  3.7259e-01,  4.3051e-01,  6.2932e-01,\n",
       "          3.1241e-01,  2.6700e-01, -3.1957e-01,  6.0621e-01,  6.3761e-01,\n",
       "         -8.9211e-02],\n",
       "        [ 1.1181e-01,  2.2471e-01,  2.1255e-01,  1.0000e+00,  2.7385e-01,\n",
       "         -2.7502e-01,  2.8679e-01,  1.2013e-01,  2.6567e-01,  3.7537e-01,\n",
       "          1.1346e-01,  3.2348e-01, -3.6480e-03,  6.8047e-02,  6.8047e-02,\n",
       "         -3.5033e-02,  3.1077e-01, -1.5678e-01,  3.6223e-02,  2.9723e-02,\n",
       "          2.5354e-01,  1.6886e-01,  3.5761e-01,  1.8055e-01,  3.4389e-01,\n",
       "          1.4069e-01,  6.6508e-02, -7.6725e-02,  3.3082e-02,  2.6567e-01,\n",
       "          4.1714e-02],\n",
       "        [ 5.6894e-01,  2.9357e-01,  8.4917e-01,  2.7385e-01,  1.0000e+00,\n",
       "         -7.2658e-02,  7.3383e-01,  2.8862e-02,  7.4284e-01,  6.6162e-01,\n",
       "          3.6692e-01,  4.6205e-01,  3.6303e-01,  4.3187e-01,  4.3187e-01,\n",
       "          5.8099e-01,  7.3498e-01,  1.4538e-01,  4.1487e-01, -2.2217e-01,\n",
       "          7.1558e-01,  7.5685e-01,  4.5624e-01,  4.2804e-01,  6.8177e-01,\n",
       "          4.1318e-01,  3.3326e-01, -3.3525e-01,  5.2034e-01,  6.5736e-01,\n",
       "          1.2057e-02],\n",
       "        [-7.3797e-02,  6.6197e-03, -1.5391e-01, -2.7502e-01, -7.2658e-02,\n",
       "          1.0000e+00,  3.4476e-04, -6.1324e-02, -9.3230e-02, -1.0216e-01,\n",
       "         -1.3729e-02, -8.4563e-02,  8.5609e-02, -1.2020e-01, -1.2020e-01,\n",
       "         -2.2689e-01, -1.1954e-01,  2.1317e-01,  1.0726e-01, -6.4058e-02,\n",
       "         -2.3464e-01, -6.7172e-02, -1.9703e-01, -3.7904e-02, -9.7204e-02,\n",
       "         -1.7894e-01,  1.4998e-01,  1.9924e-01, -7.4325e-02, -9.7980e-02,\n",
       "          1.2023e-01],\n",
       "        [ 6.0076e-01,  5.7279e-02,  7.7535e-01,  2.8679e-01,  7.3383e-01,\n",
       "          3.4476e-04,  1.0000e+00, -3.9131e-02,  7.5298e-01,  7.6425e-01,\n",
       "          4.5190e-01,  6.0335e-01,  3.4323e-01,  4.0666e-01,  4.0666e-01,\n",
       "          5.7675e-01,  6.6863e-01,  2.1327e-01,  3.7247e-01, -2.5833e-01,\n",
       "          7.0184e-01,  7.5110e-01,  2.5110e-01,  3.8675e-01,  5.3241e-01,\n",
       "          3.7550e-01,  1.5018e-01, -3.8567e-01,  5.5443e-01,  6.0379e-01,\n",
       "         -1.3771e-01],\n",
       "        [-2.9285e-02,  1.1422e-01,  5.9487e-02,  1.2013e-01,  2.8862e-02,\n",
       "         -6.1324e-02, -3.9131e-02,  1.0000e+00,  4.0905e-02,  4.6724e-02,\n",
       "         -1.6134e-01,  8.6908e-02,  9.8565e-02, -1.1993e-01, -1.1993e-01,\n",
       "          1.7619e-02, -7.7957e-02, -1.1857e-01,  1.0386e-02,  1.4956e-01,\n",
       "          1.6341e-01,  3.5715e-03,  1.4534e-01,  2.1874e-01,  2.0397e-01,\n",
       "          1.3945e-02,  3.0519e-02,  1.2358e-01, -7.9433e-02,  1.2651e-02,\n",
       "          3.9350e-01],\n",
       "        [ 7.3454e-01,  4.7118e-02,  7.7200e-01,  2.6567e-01,  7.4284e-01,\n",
       "         -9.3230e-02,  7.5298e-01,  4.0905e-02,  1.0000e+00,  7.5573e-01,\n",
       "          4.3491e-01,  5.0965e-01,  4.0886e-01,  5.9853e-01,  5.9853e-01,\n",
       "          5.2517e-01,  6.7173e-01,  2.6745e-01,  2.8606e-01, -3.0238e-01,\n",
       "          7.3283e-01,  9.2424e-01,  3.1904e-01,  4.8144e-01,  7.7676e-01,\n",
       "          3.8421e-01,  3.1537e-01, -3.9754e-01,  8.5670e-01,  7.6313e-01,\n",
       "         -2.1619e-02],\n",
       "        [ 3.3433e-01, -5.6208e-02,  7.1308e-01,  3.7537e-01,  6.6162e-01,\n",
       "         -1.0216e-01,  7.6425e-01,  4.6724e-02,  7.5573e-01,  1.0000e+00,\n",
       "          3.2549e-01,  3.6531e-01,  3.1169e-01,  3.6062e-01,  3.6062e-01,\n",
       "          4.3757e-01,  5.4535e-01,  2.2468e-01,  2.5198e-01, -3.9231e-01,\n",
       "          6.0577e-01,  7.1146e-01,  2.4100e-01,  4.4294e-01,  5.8825e-01,\n",
       "          2.5205e-01,  2.3508e-01, -3.6220e-01,  5.5856e-01,  4.4339e-01,\n",
       "         -1.3729e-01],\n",
       "        [ 3.8078e-01,  1.1162e-01,  3.1576e-01,  1.1346e-01,  3.6692e-01,\n",
       "         -1.3729e-02,  4.5190e-01, -1.6134e-01,  4.3491e-01,  3.2549e-01,\n",
       "          1.0000e+00,  3.2808e-01,  3.4181e-01,  3.9923e-01,  3.9923e-01,\n",
       "          2.0369e-01,  4.4424e-01,  6.9597e-02,  1.5181e-01, -1.1767e-01,\n",
       "          4.0867e-01,  4.3312e-01,  1.7305e-01,  1.4134e-01,  3.8191e-01,\n",
       "          2.3489e-01,  1.7058e-01, -2.3047e-01,  3.6238e-01,  4.0820e-01,\n",
       "          1.5500e-02],\n",
       "        [ 5.0355e-01,  3.8129e-01,  4.2094e-01,  3.2348e-01,  4.6205e-01,\n",
       "         -8.4563e-02,  6.0335e-01,  8.6908e-02,  5.0965e-01,  3.6531e-01,\n",
       "          3.2808e-01,  1.0000e+00,  2.5560e-01,  2.6623e-01,  2.6623e-01,\n",
       "          3.9209e-01,  5.6702e-01,  1.8572e-01,  1.4803e-01, -1.0766e-01,\n",
       "          3.8801e-01,  5.0167e-01,  1.4414e-01,  4.1451e-01,  5.0039e-01,\n",
       "          3.3244e-01,  1.3509e-01, -2.1713e-02,  4.0229e-01,  4.7920e-01,\n",
       "         -2.0464e-02],\n",
       "        [ 2.5614e-01, -6.8250e-02,  4.2314e-01, -3.6480e-03,  3.6303e-01,\n",
       "          8.5609e-02,  3.4323e-01,  9.8565e-02,  4.0886e-01,  3.1169e-01,\n",
       "          3.4181e-01,  2.5560e-01,  1.0000e+00,  3.6072e-01,  3.6072e-01,\n",
       "          3.7366e-01,  3.7312e-01,  4.4147e-01, -5.8942e-02, -1.5510e-01,\n",
       "          5.8786e-01,  5.0434e-01,  3.2926e-01,  1.4800e-01,  4.4146e-01,\n",
       "         -4.2487e-02, -2.1900e-02, -1.6750e-01,  3.8764e-01,  2.5688e-01,\n",
       "          1.2469e-01],\n",
       "        [ 4.0707e-01, -1.8178e-01,  4.2689e-01,  6.8047e-02,  4.3187e-01,\n",
       "         -1.2020e-01,  4.0666e-01, -1.1993e-01,  5.9853e-01,  3.6062e-01,\n",
       "          3.9923e-01,  2.6623e-01,  3.6072e-01,  1.0000e+00,  1.0000e+00,\n",
       "          3.0661e-01,  2.6026e-01,  1.9257e-01,  1.5606e-01, -2.4778e-01,\n",
       "          4.4655e-01,  5.0007e-01,  1.4745e-01,  3.4887e-01,  3.8930e-01,\n",
       "          1.6758e-01,  2.9228e-01, -2.7761e-01,  5.1640e-01,  3.4199e-01,\n",
       "          3.0615e-02],\n",
       "        [ 4.0707e-01, -1.8178e-01,  4.2689e-01,  6.8047e-02,  4.3187e-01,\n",
       "         -1.2020e-01,  4.0666e-01, -1.1993e-01,  5.9853e-01,  3.6062e-01,\n",
       "          3.9923e-01,  2.6623e-01,  3.6072e-01,  1.0000e+00,  1.0000e+00,\n",
       "          3.0661e-01,  2.6026e-01,  1.9257e-01,  1.5606e-01, -2.4778e-01,\n",
       "          4.4655e-01,  5.0007e-01,  1.4745e-01,  3.4887e-01,  3.8930e-01,\n",
       "          1.6758e-01,  2.9228e-01, -2.7761e-01,  5.1640e-01,  3.4199e-01,\n",
       "          3.0615e-02],\n",
       "        [ 5.0197e-01, -1.7340e-02,  6.3480e-01, -3.5033e-02,  5.8099e-01,\n",
       "         -2.2689e-01,  5.7675e-01,  1.7619e-02,  5.2517e-01,  4.3757e-01,\n",
       "          2.0369e-01,  3.9209e-01,  3.7366e-01,  3.0661e-01,  3.0661e-01,\n",
       "          1.0000e+00,  3.6976e-01,  1.5751e-01,  2.3648e-01, -1.8663e-01,\n",
       "          6.5708e-01,  6.0464e-01,  3.2899e-01,  4.0330e-01,  4.4532e-01,\n",
       "          4.1184e-01, -4.1103e-02, -3.9447e-01,  4.8141e-01,  4.6346e-01,\n",
       "         -1.2882e-01],\n",
       "        [ 6.0729e-01,  1.7689e-01,  6.6732e-01,  3.1077e-01,  7.3498e-01,\n",
       "         -1.1954e-01,  6.6863e-01, -7.7957e-02,  6.7173e-01,  5.4535e-01,\n",
       "          4.4424e-01,  5.6702e-01,  3.7312e-01,  2.6026e-01,  2.6026e-01,\n",
       "          3.6976e-01,  1.0000e+00,  1.8006e-01,  5.0128e-01, -2.4518e-01,\n",
       "          6.2168e-01,  7.1704e-01,  2.2419e-01,  2.9122e-01,  6.9212e-01,\n",
       "          4.1418e-01,  2.2975e-01, -2.5980e-01,  6.4417e-01,  6.8259e-01,\n",
       "         -1.5293e-01],\n",
       "        [ 1.3733e-01, -2.3004e-02,  2.2801e-01, -1.5678e-01,  1.4538e-01,\n",
       "          2.1317e-01,  2.1327e-01, -1.1857e-01,  2.6745e-01,  2.2468e-01,\n",
       "          6.9597e-02,  1.8572e-01,  4.4147e-01,  1.9257e-01,  1.9257e-01,\n",
       "          1.5751e-01,  1.8006e-01,  1.0000e+00, -7.3240e-02, -1.5511e-01,\n",
       "          1.9249e-01,  3.2697e-01,  1.7724e-01,  1.7007e-01,  1.6531e-01,\n",
       "         -2.1034e-02, -4.2331e-02, -1.7408e-01,  2.9624e-01,  2.1753e-01,\n",
       "         -3.1231e-02],\n",
       "        [ 4.2082e-01, -2.0182e-02,  3.6531e-01,  3.6223e-02,  4.1487e-01,\n",
       "          1.0726e-01,  3.7247e-01,  1.0386e-02,  2.8606e-01,  2.5198e-01,\n",
       "          1.5181e-01,  1.4803e-01, -5.8942e-02,  1.5606e-01,  1.5606e-01,\n",
       "          2.3648e-01,  5.0128e-01, -7.3240e-02,  1.0000e+00, -2.4496e-01,\n",
       "          2.3435e-01,  3.6648e-01,  8.3294e-02,  2.2284e-01,  2.8248e-01,\n",
       "          5.4986e-01,  3.2650e-01, -5.6948e-02,  3.2095e-01,  3.8561e-01,\n",
       "         -1.4344e-02],\n",
       "        [-1.5886e-01,  2.5926e-01, -2.5719e-01,  2.9723e-02, -2.2217e-01,\n",
       "         -6.4058e-02, -2.5833e-01,  1.4956e-01, -3.0238e-01, -3.9231e-01,\n",
       "         -1.1767e-01, -1.0766e-01, -1.5510e-01, -2.4778e-01, -2.4778e-01,\n",
       "         -1.8663e-01, -2.4518e-01, -1.5511e-01, -2.4496e-01,  1.0000e+00,\n",
       "         -2.7494e-01, -3.5418e-01,  7.1712e-03, -2.2606e-01, -1.9235e-01,\n",
       "         -1.7381e-01, -1.8482e-01,  2.3384e-01, -3.8488e-01, -5.5043e-02,\n",
       "          2.8514e-01],\n",
       "        [ 5.0278e-01, -8.6881e-02,  7.5979e-01,  2.5354e-01,  7.1558e-01,\n",
       "         -2.3464e-01,  7.0184e-01,  1.6341e-01,  7.3283e-01,  6.0577e-01,\n",
       "          4.0867e-01,  3.8801e-01,  5.8786e-01,  4.4655e-01,  4.4655e-01,\n",
       "          6.5708e-01,  6.2168e-01,  1.9249e-01,  2.3435e-01, -2.7494e-01,\n",
       "          1.0000e+00,  7.7073e-01,  4.0797e-01,  4.6589e-01,  7.1823e-01,\n",
       "          2.9552e-01,  1.3373e-01, -3.9974e-01,  5.7851e-01,  5.8274e-01,\n",
       "          7.5291e-02],\n",
       "        [ 7.0347e-01,  1.2807e-02,  7.5469e-01,  1.6886e-01,  7.5685e-01,\n",
       "         -6.7172e-02,  7.5110e-01,  3.5715e-03,  9.2424e-01,  7.1146e-01,\n",
       "          4.3312e-01,  5.0167e-01,  5.0434e-01,  5.0007e-01,  5.0007e-01,\n",
       "          6.0464e-01,  7.1704e-01,  3.2697e-01,  3.6648e-01, -3.5418e-01,\n",
       "          7.7073e-01,  1.0000e+00,  3.6469e-01,  4.6101e-01,  7.5846e-01,\n",
       "          4.2024e-01,  3.1067e-01, -4.2178e-01,  8.6261e-01,  7.3358e-01,\n",
       "         -2.4518e-02],\n",
       "        [ 3.1847e-01,  2.6289e-01,  3.7259e-01,  3.5761e-01,  4.5624e-01,\n",
       "         -1.9703e-01,  2.5110e-01,  1.4534e-01,  3.1904e-01,  2.4100e-01,\n",
       "          1.7305e-01,  1.4414e-01,  3.2926e-01,  1.4745e-01,  1.4745e-01,\n",
       "          3.2899e-01,  2.2419e-01,  1.7724e-01,  8.3294e-02,  7.1712e-03,\n",
       "          4.0797e-01,  3.6469e-01,  1.0000e+00,  5.9764e-02,  3.8280e-01,\n",
       "          3.5061e-01, -3.2257e-02, -1.5769e-01,  1.6576e-01,  4.5307e-01,\n",
       "          1.6057e-01],\n",
       "        [ 3.8264e-01,  1.4439e-01,  4.3051e-01,  1.8055e-01,  4.2804e-01,\n",
       "         -3.7904e-02,  3.8675e-01,  2.1874e-01,  4.8144e-01,  4.4294e-01,\n",
       "          1.4134e-01,  4.1451e-01,  1.4800e-01,  3.4887e-01,  3.4887e-01,\n",
       "          4.0330e-01,  2.9122e-01,  1.7007e-01,  2.2284e-01, -2.2606e-01,\n",
       "          4.6589e-01,  4.6101e-01,  5.9764e-02,  1.0000e+00,  3.9636e-01,\n",
       "          2.9517e-01,  2.4855e-01, -9.8872e-02,  4.0377e-01,  3.5574e-01,\n",
       "          4.8670e-02],\n",
       "        [ 5.0305e-01,  8.7400e-02,  6.2932e-01,  3.4389e-01,  6.8177e-01,\n",
       "         -9.7204e-02,  5.3241e-01,  2.0397e-01,  7.7676e-01,  5.8825e-01,\n",
       "          3.8191e-01,  5.0039e-01,  4.4146e-01,  3.8930e-01,  3.8930e-01,\n",
       "          4.4532e-01,  6.9212e-01,  1.6531e-01,  2.8248e-01, -1.9235e-01,\n",
       "          7.1823e-01,  7.5846e-01,  3.8280e-01,  3.9636e-01,  1.0000e+00,\n",
       "          2.2134e-01,  4.1122e-01, -3.0626e-01,  7.0406e-01,  6.6971e-01,\n",
       "          1.7213e-01],\n",
       "        [ 5.8528e-01,  1.9350e-01,  3.1241e-01,  1.4069e-01,  4.1318e-01,\n",
       "         -1.7894e-01,  3.7550e-01,  1.3945e-02,  3.8421e-01,  2.5205e-01,\n",
       "          2.3489e-01,  3.3244e-01, -4.2487e-02,  1.6758e-01,  1.6758e-01,\n",
       "          4.1184e-01,  4.1418e-01, -2.1034e-02,  5.4986e-01, -1.7381e-01,\n",
       "          2.9552e-01,  4.2024e-01,  3.5061e-01,  2.9517e-01,  2.2134e-01,\n",
       "          1.0000e+00,  8.2642e-02, -7.4871e-02,  3.3740e-01,  5.3744e-01,\n",
       "         -5.6014e-02],\n",
       "        [ 1.7646e-01, -4.3373e-02,  2.6700e-01,  6.6508e-02,  3.3326e-01,\n",
       "          1.4998e-01,  1.5018e-01,  3.0519e-02,  3.1537e-01,  2.3508e-01,\n",
       "          1.7058e-01,  1.3509e-01, -2.1900e-02,  2.9228e-01,  2.9228e-01,\n",
       "         -4.1103e-02,  2.2975e-01, -4.2331e-02,  3.2650e-01, -1.8482e-01,\n",
       "          1.3373e-01,  3.1067e-01, -3.2257e-02,  2.4855e-01,  4.1122e-01,\n",
       "          8.2642e-02,  1.0000e+00, -2.4623e-03,  2.3914e-01,  2.0180e-01,\n",
       "          9.8647e-02],\n",
       "        [-1.8621e-01,  1.9095e-01, -3.1957e-01, -7.6725e-02, -3.3525e-01,\n",
       "          1.9924e-01, -3.8567e-01,  1.2358e-01, -3.9754e-01, -3.6220e-01,\n",
       "         -2.3047e-01, -2.1713e-02, -1.6750e-01, -2.7761e-01, -2.7761e-01,\n",
       "         -3.9447e-01, -2.5980e-01, -1.7408e-01, -5.6948e-02,  2.3384e-01,\n",
       "         -3.9974e-01, -4.2178e-01, -1.5769e-01, -9.8872e-02, -3.0626e-01,\n",
       "         -7.4871e-02, -2.4623e-03,  1.0000e+00, -3.9450e-01, -2.8824e-01,\n",
       "         -8.4697e-02],\n",
       "        [ 7.0743e-01, -1.1387e-01,  6.0621e-01,  3.3082e-02,  5.2034e-01,\n",
       "         -7.4325e-02,  5.5443e-01, -7.9433e-02,  8.5670e-01,  5.5856e-01,\n",
       "          3.6238e-01,  4.0229e-01,  3.8764e-01,  5.1640e-01,  5.1640e-01,\n",
       "          4.8141e-01,  6.4417e-01,  2.9624e-01,  3.2095e-01, -3.8488e-01,\n",
       "          5.7851e-01,  8.6261e-01,  1.6576e-01,  4.0377e-01,  7.0406e-01,\n",
       "          3.3740e-01,  2.3914e-01, -3.9450e-01,  1.0000e+00,  7.2457e-01,\n",
       "         -1.6085e-01],\n",
       "        [ 8.5510e-01,  2.6632e-01,  6.3761e-01,  2.6567e-01,  6.5736e-01,\n",
       "         -9.7980e-02,  6.0379e-01,  1.2651e-02,  7.6313e-01,  4.4339e-01,\n",
       "          4.0820e-01,  4.7920e-01,  2.5688e-01,  3.4199e-01,  3.4199e-01,\n",
       "          4.6346e-01,  6.8259e-01,  2.1753e-01,  3.8561e-01, -5.5043e-02,\n",
       "          5.8274e-01,  7.3358e-01,  4.5307e-01,  3.5574e-01,  6.6971e-01,\n",
       "          5.3744e-01,  2.0180e-01, -2.8824e-01,  7.2457e-01,  1.0000e+00,\n",
       "         -2.9809e-02],\n",
       "        [-1.1885e-01,  2.1493e-01, -8.9211e-02,  4.1714e-02,  1.2057e-02,\n",
       "          1.2023e-01, -1.3771e-01,  3.9350e-01, -2.1619e-02, -1.3729e-01,\n",
       "          1.5500e-02, -2.0464e-02,  1.2469e-01,  3.0615e-02,  3.0615e-02,\n",
       "         -1.2882e-01, -1.5293e-01, -3.1231e-02, -1.4344e-02,  2.8514e-01,\n",
       "          7.5291e-02, -2.4518e-02,  1.6057e-01,  4.8670e-02,  1.7213e-01,\n",
       "         -5.6014e-02,  9.8647e-02, -8.4697e-02, -1.6085e-01, -2.9809e-02,\n",
       "          1.0000e+00]])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(F.normalize(vA[i]),F.normalize(vA[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T18:16:23.371887Z",
     "start_time": "2021-07-22T18:16:23.362155Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 32])"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((4, 3, 32))\n",
    "x\n",
    "x = F.normalize(x, dim=0, p=2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T18:12:51.451956Z",
     "start_time": "2021-07-22T18:12:51.441994Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3910,  0.6785,  0.6430,  ...,  0.6427,  0.4616,  0.7578],\n",
       "        [ 0.4815,  0.5720,  0.4883,  ...,  0.3487,  0.6591,  0.5531],\n",
       "        [-0.1119, -0.2948, -0.2491,  ..., -0.0994, -0.2847, -0.1486],\n",
       "        ...,\n",
       "        [-0.1119, -0.2948, -0.2491,  ..., -0.0994, -0.2847, -0.1486],\n",
       "        [ 0.1997,  0.4433,  0.6723,  ...,  0.4092,  0.5331,  0.5286],\n",
       "        [ 0.2680,  0.3634,  0.3680,  ...,  0.3556,  0.2484,  0.4183]])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T18:14:20.473333Z",
     "start_time": "2021-07-22T18:14:20.462940Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0946,  0.1642,  0.1556,  0.2153,  0.1043,  0.1197,  0.1314,  0.0906,\n",
       "         0.1719,  0.1469,  0.1738,  0.1456,  0.0867,  0.0175,  0.1745,  0.1191,\n",
       "         0.1545,  0.0821,  0.1410, -0.1081,  0.1075,  0.0407,  0.1045,  0.1702,\n",
       "         0.1156,  0.1103,  0.1524,  0.1199,  0.1738,  0.1290,  0.0066,  0.0918,\n",
       "         0.1095,  0.1372,  0.1372,  0.0785,  0.1056,  0.1402,  0.1935,  0.0942,\n",
       "         0.0620,  0.0552,  0.1264,  0.0909,  0.0365, -0.0589,  0.1300,  0.1306,\n",
       "         0.0785,  0.1878,  0.1375,  0.1336,  0.1744,  0.0718,  0.1149, -0.0243,\n",
       "         0.0704,  0.1107,  0.1905,  0.1555,  0.1117,  0.1833])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.normalize(matrix[0][0], dim=0, p=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T08:58:53.866872Z",
     "start_time": "2021-07-22T08:58:53.852862Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2184,  0.0017,  0.2983,  ...,  0.4810, -0.1870,  0.2915],\n",
       "        [-0.0558, -0.1941,  0.1702,  ...,  0.1114, -0.1398,  0.0626],\n",
       "        [ 0.0960,  0.1344, -0.1712,  ..., -0.0192, -0.1004, -0.1435],\n",
       "        ...,\n",
       "        [ 0.1518, -0.0945,  0.4693,  ...,  0.6139, -0.0319,  0.1021],\n",
       "        [ 0.2096, -0.1173,  0.4186,  ...,  0.4406,  0.0737,  0.1675],\n",
       "        [-0.2742, -0.1711, -0.0318,  ..., -0.2129,  1.0000, -0.1324]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-8)\n",
    "matrix = cos(vA[1].unsqueeze(1), vB[1])\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T08:59:50.357988Z",
     "start_time": "2021-07-22T08:59:50.350739Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1712)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(vA[1][2], vB[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T08:55:49.983600Z",
     "start_time": "2021-07-22T08:55:49.976644Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0605, -0.7688,  0.1739, -0.3395,  0.8663,  0.3894,  0.1340, -0.8580,\n",
       "        -1.6964,  1.0345,  0.6534, -0.6326,  1.1859, -0.2747, -0.0704,  0.5240,\n",
       "         0.9124,  0.0323,  0.1340,  0.1626, -0.4611,  0.4656,  0.5794,  0.1429,\n",
       "        -0.9366,  1.1851, -0.6629, -0.7722,  0.4630, -0.8474, -0.8556,  0.2715,\n",
       "        -0.1746, -0.4981, -0.7126, -0.0603, -0.2227, -0.7841,  0.3040,  0.2132,\n",
       "         0.3704, -1.0180,  0.5923,  1.4925,  1.0837,  0.3690, -1.0705,  1.0322,\n",
       "        -0.4234, -0.4439])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vA[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T08:55:35.053560Z",
     "start_time": "2021-07-22T08:55:35.046266Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1712)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[1][2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T16:15:40.909810Z",
     "start_time": "2021-07-22T16:15:40.902596Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class GaussianKernel(torch.nn.Module):\n",
    "    def __init__(self, mu: float = 1., sigma: float = 1.):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_sq = (x - self.mu)**2\n",
    "        return np.exp(-norm_sq/(2*self.sigma**2)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T16:16:33.755859Z",
     "start_time": "2021-07-22T16:16:33.751082Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gk = GaussianKernel()\n",
    "\n",
    "s= gk(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T16:16:44.874041Z",
     "start_time": "2021-07-22T16:16:44.866522Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([46.6277, 40.5472, 35.1681, 46.8113, 42.9065, 49.8048, 39.1281, 48.8337,\n",
       "        49.8400, 45.2312, 44.5298, 41.0952, 47.4167, 47.4946, 43.4154, 45.3958,\n",
       "        46.0130, 45.3535, 35.6942, 48.3856, 40.4608, 50.3658, 50.1863, 49.5398,\n",
       "        50.7326, 44.7961, 44.4965, 46.6538, 49.8793, 48.1080, 34.9131])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T22:00:07.784964Z",
     "start_time": "2021-07-21T22:00:07.679126Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "seed = 42\n",
    "min_group_size = 2\n",
    "fill_top_to = 3\n",
    "\n",
    "inp_df = s.glue_train_df.copy()\n",
    "inp_df_select = inp_df[['id_left', 'id_right', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T22:00:10.087157Z",
     "start_time": "2021-07-21T22:00:09.108174Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atuthvatullin/environments/albert/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "t = inp_df_select.merge(inp_df_select, how='left', on='id_left')\n",
    "tt = t[(t.label_x==1)&(t.label_y==0)]\n",
    "tt['ones'] = 1\n",
    "ttt = tt[['id_left', 'id_right_x', 'id_right_y', 'ones']].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inp_df_select_unr = inp_df_select.copy()\n",
    "inp_df_select_unr.loc[:,'id_right'] = inp_df_select_unr['id_right'].sample(frac=1, random_state=42).values\n",
    "\n",
    "unr = inp_df_select_unr.merge(inp_df_select, on=['id_left', 'id_right'], how='left')\n",
    "unr = unr[unr.label_y.isnull()]\n",
    "\n",
    "ll = inp_df_select.merge(unr, how='left', on='id_left')\n",
    "ll['ones'] = 1\n",
    "lll = ll[['id_left', 'id_right_x', 'id_right_y', 'ones']].values\n",
    "\n",
    "itog = np.concatenate((ttt,lll))\n",
    "\n",
    "np.random.seed(seed)\n",
    "# np.random.shuffle(itog)\n",
    "itog_ones_ind = np.random.choice(len(itog), size=6000, replace=False)\n",
    "itog_ones = itog[itog_ones_ind]\n",
    "itog_ones = itog_ones.tolist()\n",
    "\n",
    "itog_zeros = []\n",
    "itog_zeros_ind = np.random.choice(len(itog), size=4000, replace=False)\n",
    "for i in itog_zeros_ind:\n",
    "    tl = itog[i]\n",
    "    q1 = tl[0]\n",
    "    q2 = tl[2]\n",
    "    q3 = tl[1]\n",
    "    itog_zeros.append([q1, q2, q3, 0])\n",
    "\n",
    "final = itog_ones + itog_zeros\n",
    "np.random.shuffle(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "s.train(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
