{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T11:08:17.236691Z",
     "start_time": "2021-08-01T11:08:17.232760Z"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pip install langdetect==1.0.8\n",
    "# !pip install flask==1.1.2\n",
    "# !pip install faiss-cpu==1.7.0\n",
    "# !pip install catboost==0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T13:41:25.381156Z",
     "start_time": "2021-09-13T13:41:23.853877Z"
    }
   },
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import jsonify, request\n",
    "\n",
    "import os\n",
    "import string\n",
    "from typing import Dict, List, Tuple, Union, Callable\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import faiss\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernel(torch.nn.Module):\n",
    "    def __init__(self, mu: float = 1., sigma: float = 1.):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(\n",
    "            -0.5 * ((x - self.mu) ** 2) / (self.sigma ** 2)\n",
    "        )\n",
    "\n",
    "class KNRM(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 emb_path: str, \n",
    "                 mlp_path: str,\n",
    "                 kernel_num: int = 21,\n",
    "                 sigma: float = 0.1, \n",
    "                 exact_sigma: float = 0.001,\n",
    "                 out_layers: List[int] = []\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(\n",
    "            torch.load(emb_path)['weight'],\n",
    "            freeze=True,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.kernel_num = kernel_num\n",
    "        self.out_layers = out_layers\n",
    "        self.sigma = sigma\n",
    "        self.exact_sigma = exact_sigma\n",
    "\n",
    "        self.kernels = self._get_kernels_layers()\n",
    "        self.mlp = self._get_mlp()\n",
    "        self.mlp.load_state_dict(torch.load(mlp_path))\n",
    "        self.out_activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def _get_kernels_layers(self) -> torch.nn.ModuleList:\n",
    "        kernels = torch.nn.ModuleList()\n",
    "        for i in range(self.kernel_num):\n",
    "            mu = 1. / (self.kernel_num - 1) + (2. * i) / (\n",
    "                self.kernel_num - 1) - 1.0\n",
    "            sigma = self.sigma\n",
    "            if mu > 1.0:\n",
    "                sigma = self.exact_sigma\n",
    "                mu = 1.0\n",
    "            kernels.append(GaussianKernel(mu=mu, sigma=sigma))\n",
    "        return kernels\n",
    "\n",
    "    def _get_mlp(self) -> torch.nn.Sequential:\n",
    "        out_cont = [self.kernel_num] + self.out_layers + [1]\n",
    "        mlp = [\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(in_f, out_f),\n",
    "                torch.nn.ReLU()\n",
    "            )\n",
    "            for in_f, out_f in zip(out_cont, out_cont[1:])\n",
    "        ]\n",
    "        mlp[-1] = mlp[-1][:-1]\n",
    "        return torch.nn.Sequential(*mlp)\n",
    "\n",
    "    def _get_matching_matrix(self, query: torch.Tensor, doc: torch.Tensor) -> torch.FloatTensor:\n",
    "        # shape = [B, L, D]\n",
    "        embed_query = self.embeddings(query.long())\n",
    "        # shape = [B, R, D]\n",
    "        embed_doc = self.embeddings(doc.long())\n",
    "\n",
    "        # shape = [B, L, R]\n",
    "        matching_matrix = torch.einsum(\n",
    "            'bld,brd->blr',\n",
    "            F.normalize(embed_query, p=2, dim=-1),\n",
    "            F.normalize(embed_doc, p=2, dim=-1)\n",
    "        )\n",
    "        return matching_matrix\n",
    "\n",
    "    def _apply_kernels(self, matching_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        KM = []\n",
    "        for kernel in self.kernels:\n",
    "            # shape = [B]\n",
    "            K = torch.log1p(kernel(matching_matrix).sum(dim=-1)).sum(dim=-1)\n",
    "            KM.append(K)\n",
    "\n",
    "        # shape = [B, K]\n",
    "        kernels_out = torch.stack(KM, dim=1)\n",
    "        return kernels_out\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        query, doc = inputs['query'], inputs['document']\n",
    "        # shape = [B, L, R]\n",
    "        matching_matrix = self._get_matching_matrix(query, doc)\n",
    "        # shape [B, K]\n",
    "        kernels_out = self._apply_kernels(matching_matrix)\n",
    "        # shape [B]\n",
    "        out = self.mlp(kernels_out)\n",
    "        return out\n",
    "\n",
    "app = Flask(__name__)\n",
    "model_is_ready = False\n",
    "index_is_ready = False\n",
    "\n",
    "class Helper:\n",
    "    def __init__(self):\n",
    "        self.emb_path_glove = os.environ['EMB_PATH_GLOVE']\n",
    "        self.vocab_path = os.environ['VOCAB_PATH']\n",
    "        self.emb_path_knrm = os.environ['EMB_PATH_KNRM']\n",
    "        self.mlp_path = os.environ['MLP_PATH']\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "    def prepare_model(self):\n",
    "        self.model = KNRM(\n",
    "            emb_path=self.emb_path_knrm,\n",
    "            mlp_path=self.mlp_path\n",
    "        )\n",
    "        with open(self.vocab_path, 'r') as f_in:\n",
    "            self.vocab = json.load(f_in)\n",
    "        global model_is_ready\n",
    "        model_is_ready = True\n",
    "\n",
    "    def _hadle_punctuation(self, inp_str: str) -> str:\n",
    "        inp_str = str(inp_str)\n",
    "        for punct in string.punctuation:\n",
    "            inp_str = inp_str.replace(punct, ' ')\n",
    "        return inp_str\n",
    "    \n",
    "    def _simple_preproc(self, inp_str: str):\n",
    "        base_str = inp_str.strip().lower()\n",
    "        str_wo_punct = self._hadle_punctuation(base_str)\n",
    "        return nltk.word_tokenize(str_wo_punct)\n",
    "\n",
    "    def prepare_index(self, documents: Dict[str, str]):\n",
    "        oov_val = self.vocab[\"OOV\"]\n",
    "        self.documents = documents\n",
    "        idxs, docs = [], []\n",
    "        for idx in documents:\n",
    "            idxs.append(int(idx)) # извлекаем документы и их индексы, чтобы делать мапинг\n",
    "            docs.append(documents[idx])\n",
    "        embeddings = []\n",
    "        emb_layer = self.model.embeddings.state_dict()['weight'] # embeddings - матрица эмбедингов KNRM (N*D)\n",
    "        for d in docs:\n",
    "            tmp_emb = [self.vocab.get(w, oov_val) for w in self._simple_preproc(d)]\n",
    "            tmp_emb = emb_layer[tmp_emb].mean(dim = 0) # взятие среднего эмбединга по всему предложению\n",
    "#             как вариант можно сделать атеншн - конкатинировать средний вектор предложения и вектор токена\n",
    "# нейрока будет предсказывать вес слова в контексте (аналог tf-idf)\n",
    "\n",
    "# можно нормировать эмбединги, обычно это улучает скор\n",
    "\n",
    "        embeddings.append(np.array(tmp_emb))          \n",
    "        embeddings = np.array([embedding for embedding in embeddings]).astype(np.float32) # кастим все в float32, иначе faiss ругается\n",
    "        self.index = faiss.IndexFlatL2(embeddings.shape[1]) # делаем brute force поиск, не приближенный (knn с l2 расстоянием)\n",
    "        self.index = faiss.IndexIDMap(self.index) # можем возращать не номера а идентификаторы соседей (у нас они есть)\n",
    "        self.index.add_with_ids(embeddings, np.array(idxs))\n",
    "        index_size = self.index.ntotal\n",
    "        global index_is_ready\n",
    "        index_is_ready = True\n",
    "        \n",
    "        return index_size\n",
    "\n",
    "    def _text_to_token_ids(self, text_list: List[str]):\n",
    "        tokenized = []\n",
    "        for text in text_list:\n",
    "            tokenized_text = self._simple_preproc(text)\n",
    "            token_idxs = [self.vocab.get(i, self.vocab[\"OOV\"]) for i in tokenized_text]\n",
    "            tokenized.append(token_idxs)\n",
    "        max_len = max(len(elem) for elem in tokenized)\n",
    "        tokenized = [elem + [0] * (max_len - len(elem)) for elem in tokenized] # делаем padding до длины самого длинного предложения\n",
    "        tokenized = torch.LongTensor(tokenized)    \n",
    "        return tokenized\n",
    "\n",
    "    def get_suggestion(self, \n",
    "            query: str, ret_k: int = 10, \n",
    "            ann_k: int = 100) -> List[Tuple[str, str]]: # набор кандидатов которые возращает faiss\n",
    "#         100 кандидатов рескорим через knrm и выдаем 10\n",
    "        q_tokens = self._simple_preproc(query)\n",
    "        vector = [self.vocab.get(tok, self.vocab[\"OOV\"]) for tok in q_tokens]\n",
    "        emb_layer = self.model.embeddings.state_dict()['weight']\n",
    "        q_emb = emb_layer[vector].mean(dim = 0).reshape(1, -1)\n",
    "        q_emb = np.array(q_emb).astype(np.float32)\n",
    "        _, I = self.index.search(q_emb, k = ann_k)\n",
    "        cands = [(str(i), self.documents[str(i)]) for i in I[0] if i != -1] # мапинг из индекса в документ\n",
    "        inputs = dict()\n",
    "        inputs['query'] = self._text_to_token_ids([query] * len(cands)) # множим запрос на кол-во кандидатов для knrm\n",
    "        inputs['document'] = self._text_to_token_ids([cnd[1] for cnd in cands]) # берем только документ\n",
    "#         в итоге получаем словарь где значения - longtensor - индексы токенов для для запроса и документа\n",
    "        scores = self.model(inputs)\n",
    "        res_ids = scores.reshape(-1).argsort(descending=True)\n",
    "        res_ids = res_ids[:ret_k]\n",
    "        res = [cands[i] for i in res_ids.tolist()] # обратный мапинг\n",
    "        return res\n",
    "\n",
    "    def query_handler(self, inp):\n",
    "        input_json = json.loads(inp.json)\n",
    "        queries = input_json[\"queries\"]\n",
    "        lang_check = []\n",
    "        suggestions = []\n",
    "        for q in queries:\n",
    "            is_en = detect(q) == \"en\"\n",
    "            lang_check.append(is_en)\n",
    "            if not is_en:\n",
    "                suggestions.append(None)\n",
    "                continue\n",
    "            suggestion = self.get_suggestion(q)\n",
    "            suggestions.append(suggestion)\n",
    "        return suggestions, lang_check\n",
    "\n",
    "    def index_handler(self, inp): # создать индекс по набору документов\n",
    "        input_json = json.loads(inp.json)\n",
    "        documents = input_json[\"documents\"]\n",
    "        index_size = self.prepare_index(documents)\n",
    "        return index_size\n",
    "\n",
    "hlp = Helper() # заложена основная логика\n",
    "\n",
    "@app.route('/ping')\n",
    "def ping():\n",
    "    if not model_is_ready:\n",
    "        return jsonify(status=\"not ready\")\n",
    "    return jsonify(status=\"ok\")\n",
    "\n",
    "@app.route('/query', methods=['POST'])\n",
    "def query(): # запросы перегоняем в усредн эмбед, ищем кандидатов, строим полноценные эмбед для каждого слова, \n",
    "#     прогоняем knrm и получаем топ-10 кандидатов\n",
    "    if not model_is_ready or not index_is_ready:\n",
    "        return json.dumps({\"status\": \"FAISS is not initialized!\"}) # json.dumps == jsonify\n",
    "    suggestions, lang_check = hlp.query_handler(request)\n",
    "\n",
    "    return jsonify(suggestions=suggestions, lang_check=lang_check)\n",
    "\n",
    "@app.route('/update_index', methods=['POST'])\n",
    "def update_index():\n",
    "    index_size = hlp.index_handler(request)\n",
    "\n",
    "    return jsonify(status=\"ok\", index_size=index_size)\n",
    "\n",
    "hlp.prepare_model()\n",
    "\n",
    "# можно сделать многоуровневую систему бм25 - 10000, потом w2v - 100, потом bert - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import jsonify, request\n",
    "\n",
    "def antimat_model(input_text):\n",
    "    output_text = \"Добрый день! \" + input_text\n",
    "    return output_text\n",
    "\n",
    "@app.route('/antimat', methods=['POST'])\n",
    "def update_index():\n",
    "    input_json = json.loads(request.json)\n",
    "    input_text = input_json[\"Text\"]\n",
    "    refined_text = antimat_model(input_text)\n",
    "    return jsonify(status=\"ok\", RefinedText=refined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T13:45:39.881973Z",
     "start_time": "2021-09-13T13:45:39.877055Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_mlp(kernel_num=21, out_layers=[]) -> torch.nn.Sequential:\n",
    "    out_cont = [kernel_num] + out_layers + [1]\n",
    "    mlp = [\n",
    "        torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_f, out_f),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        for in_f, out_f in zip(out_cont, out_cont[1:])\n",
    "    ]\n",
    "    mlp[-1] = mlp[-1][:-1]\n",
    "    return torch.nn.Sequential(*mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T14:51:33.154707Z",
     "start_time": "2021-09-13T14:51:33.150021Z"
    }
   },
   "outputs": [],
   "source": [
    "mlp_path = 'mlp_2.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(input):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = [['мама', \"мыла\", \"раму\"],['a', 'b', 'c']]\n",
    "unique_words = [i for sl in ll for i in sl]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T14:51:33.483047Z",
     "start_time": "2021-09-13T14:51:33.470082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.0.weight',\n",
       "              tensor([[-0.0017,  0.1168, -0.1810, -0.1642, -0.0898,  0.0357,  0.0042,  0.2626,\n",
       "                        0.1491,  0.2527,  0.1289,  0.1276, -0.0758, -0.0461, -0.0433, -0.0088,\n",
       "                        0.0063,  0.0772,  0.0238,  0.5173,  1.0399]])),\n",
       "             ('0.0.bias', tensor([0.1812]))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(mlp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T14:51:35.456823Z",
     "start_time": "2021-09-13T14:51:35.446221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = _get_mlp()\n",
    "mlp.load_state_dict(torch.load(mlp_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T14:00:35.318770Z",
     "start_time": "2021-09-13T14:00:35.307495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.0017,  0.1167, -0.1811, -0.1646, -0.0907,  0.0333, -0.0210,  0.1374,\n",
       "                       -0.0062,  0.0940, -0.0296, -0.0269, -0.1976, -0.0713, -0.0418, -0.0026,\n",
       "                        0.0133,  0.0857,  0.0418,  0.5564,  1.0941]])),\n",
       "             ('0.bias', tensor([0.1812]))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(mlp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T14:00:02.455396Z",
     "start_time": "2021-09-13T14:00:02.447171Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "d = OrderedDict([('0.0.weight',\n",
    "              torch.Tensor([[-0.0017,  0.1167, -0.1811, -0.1646, -0.0907,  0.0333, -0.0210,  0.1374,\n",
    "                       -0.0062,  0.0940, -0.0296, -0.0269, -0.1976, -0.0713, -0.0418, -0.0026,\n",
    "                        0.0133,  0.0857,  0.0418,  0.5564,  1.0941]])),\n",
    "             ('0.0.bias', torch.Tensor([0.1812]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T14:00:02.838513Z",
     "start_time": "2021-09-13T14:00:02.830403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.load_state_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T13:58:52.201145Z",
     "start_time": "2021-09-13T13:58:52.195834Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EMB_PATH_KNRM = 'knrm_embed.bin'\n",
    "VOCAB_PATH = 'vocab.json'\n",
    "MLP_PATH = 'mlp.bin'\n",
    "EMB_PATH_GLOVE = 'glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T07:47:47.278699Z",
     "start_time": "2021-07-29T07:47:47.269143Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/ping')\n",
    "def ping():\n",
    "    return Response(status = 200)\n",
    "\n",
    "\n",
    "@app.route('/load')\n",
    "def load():\n",
    "    content = json.loads(request.json)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T07:29:07.327350Z",
     "start_time": "2021-07-29T07:24:27.247075Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"solution.py\"\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:11000/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!FLASK_APP=user_input/solution.py flask run --port 11000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
