{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T13:45:07.921095Z",
     "start_time": "2021-07-12T13:45:07.864691Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from catboost.datasets import msrank_10k\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ListNet(torch.nn.Module):\n",
    "    def __init__(self, num_input_features: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_input_features, self.hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_1: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self.model(input_1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def __init__(self, n_epochs: int = 5, listnet_hidden_dim: int = 30,\n",
    "                 lr: float = 0.001, ndcg_top_k: int = 10):\n",
    "        self._prepare_data()\n",
    "        self.num_input_features = self.X_train.shape[1]\n",
    "        self.ndcg_top_k = ndcg_top_k\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.model = self._create_model(\n",
    "            self.num_input_features, listnet_hidden_dim)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def _get_data(self) -> List[np.ndarray]:\n",
    "        train_df, test_df = msrank_10k()\n",
    "\n",
    "        X_train = train_df.drop([0, 1], axis=1).values\n",
    "        y_train = train_df[0].values\n",
    "        query_ids_train = train_df[1].values.astype(int)\n",
    "\n",
    "        X_test = test_df.drop([0, 1], axis=1).values\n",
    "        y_test = test_df[0].values\n",
    "        query_ids_test = test_df[1].values.astype(int)\n",
    "\n",
    "        return [X_train, y_train, query_ids_train, X_test, y_test, query_ids_test]\n",
    "\n",
    "    def _prepare_data(self) -> None:\n",
    "        (X_train, y_train, self.query_ids_train,\n",
    "            X_test, y_test, self.query_ids_test) = self._get_data()\n",
    "\n",
    "        self.X_train = self._scale_features_in_query_groups(X_train, self.query_ids_train)\n",
    "        self.X_test = self._scale_features_in_query_groups(X_test, self.query_ids_test)\n",
    "\n",
    "        self.ys_train = torch.FloatTensor(y_train)\n",
    "        self.ys_test = torch.FloatTensor(y_test)\n",
    "\n",
    "    \n",
    "#     def _scale_features_in_query_groups(self, inp_feat_array: np.ndarray,\n",
    "#                                         inp_query_ids: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "#         total_inp_scaled = np.empty((0, inp_feat_array.shape[1])) \n",
    "        \n",
    "#         total_ids_list = []\n",
    "        \n",
    "#         for for_v in np.unique(inp_query_ids):\n",
    "#             ids_list = [i for i, v in enumerate(inp_query_ids) if v==for_v]\n",
    "            \n",
    "            \n",
    "#             scaler = StandardScaler()\n",
    "            \n",
    "#             group_inp_scaled = scaler.fit_transform(inp_feat_array[ids_list])\n",
    "                        \n",
    "#             total_inp_scaled = np.concatenate([total_inp_scaled, \n",
    "#                                                group_inp_scaled])\n",
    "    \n",
    "#             total_ids_list.append(ids_list)\n",
    "        \n",
    "#         total_ids_list = [i for sl in total_ids_list for i in sl]\n",
    "            \n",
    "#         sort_ind = np.argsort(total_ids_list)\n",
    "        \n",
    "#         total_inp_scaled = torch.FloatTensor(total_inp_scaled[sort_ind])\n",
    "     \n",
    "#         return total_inp_scaled\n",
    "    \n",
    "    # ideal\n",
    "    def _scale_features_in_query_groups(self, inp_feat_array: np.ndarray,\n",
    "                                        inp_query_ids: np.ndarray) -> np.ndarray:\n",
    "        for cur_id in np.unique(inp_query_ids):\n",
    "            mask = inp_query_ids == cur_id\n",
    "            tmp_array = inp_feat_array[mask]\n",
    "            scaler = StandardScaler()\n",
    "            inp_feat_array[mask] = scaler.fit_transform(tmp_array)\n",
    "\n",
    "        return inp_feat_array\n",
    "\n",
    "\n",
    "    def _create_model(self, listnet_num_input_features: int,\n",
    "                      listnet_hidden_dim: int) -> torch.nn.Module:\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        net = ListNet(listnet_num_input_features, listnet_hidden_dim)\n",
    "        \n",
    "        return net\n",
    "    \n",
    "\n",
    "#     def fit(self) -> List[float]:\n",
    "        \n",
    "#         score_list = []\n",
    "        \n",
    "#         for e in range(self.n_epochs):\n",
    "            \n",
    "#             N_train = len(self.X_train)\n",
    "#             idx = torch.randperm(N_train) # шафлим чтобы 1 батч 1й эпохи отличался от 1 бача во 2й\n",
    "#             self.X_train = self.X_train[idx]\n",
    "#             self.ys_train = self.ys_train[idx]\n",
    "        \n",
    "#             print(f'epoch {e+1}')\n",
    "#             self._train_one_epoch()\n",
    "#             score_ep = self._eval_test_set()\n",
    "#             print(f'score on eval set: {round(score_ep, 4)}')\n",
    "#             score_list.append(score_ep)\n",
    "#             print('\\n')\n",
    "            \n",
    "#         return score_list\n",
    "    \n",
    "    # ideal\n",
    "    def fit(self) -> List[float]:\n",
    "        metrics = []\n",
    "        # нужны ли пермутации?\n",
    "        for epoch_no in range(1, self.n_epochs + 1):\n",
    "            self._train_one_epoch()\n",
    "            ep_metric = self._eval_test_set()\n",
    "            metrics.append(ep_metric)\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def _train_one_epoch(self) -> None:\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "        for train_id_cat in np.unique(self.query_ids_train):\n",
    "\n",
    "#             idx_list = [i for i, v in enumerate(self.query_ids_train) if v==train_id_cat]\n",
    "#             batch_X = self.X_train[idx_list]\n",
    "#             batch_ys = self.ys_train[idx_list]\n",
    "            \n",
    "            mask_train = self.query_ids_train == train_id_cat\n",
    "            batch_X = self.X_train[mask_train]\n",
    "            batch_ys = self.ys_train[mask_train]\n",
    "            \n",
    "            self.optimizer.zero_grad() # чтобы не накапливать градиенты\n",
    "\n",
    "            batch_pred = self.model(batch_X).reshape(-1, ) \n",
    "            # не нужно прописывать model.forward тк при наследовании от класса torch.nn.Module метод __call__ заменяется на метод forward\n",
    "            batch_loss = self._calc_loss(batch_ys, batch_pred)\n",
    "\n",
    "            batch_loss.backward(retain_graph=True) # возвращаемый тензор может прокидывать через себя градиенты (batch_loss)\n",
    "            # backward рассчитывает градиенты, накапливаются градиенты, по которым мы можем делать шаг градиентного спуска (step)\n",
    "#             retain_graph=True часто не нужен\n",
    "            self.optimizer.step()\n",
    "\n",
    "    \n",
    "    def _eval_test_set(self) -> float:\n",
    "        with torch.no_grad(): # заставляем торч не рассчитывать градиенты\n",
    "            self.model.eval() \n",
    "            # в данном примере не на что не влияет потому что в модели нет слоев которые изменяются на этапе обучения и предсказания(типа dropout, batchnorm)\n",
    "            # dropout рандомно зануляет признаки симулирую их выкидывание - чтобы не переобучаться, для генерализации\n",
    "            # eval - при предсказании не нужно выбрасывать признаки\n",
    "            ndcgs = []\n",
    "            for test_id_cat in np.unique(self.query_ids_test):\n",
    "\n",
    "                idx_list = [i for i, v in enumerate(self.query_ids_test) if v==test_id_cat]\n",
    "\n",
    "                valid_pred = self.model(self.X_test[idx_list])\n",
    "                ys_valid = self.ys_test[idx_list]\n",
    "\n",
    "                ndcg_score = self._ndcg_k(ys_valid, valid_pred, self.ndcg_top_k)\n",
    "\n",
    "                ndcgs.append(ndcg_score)\n",
    "                \n",
    "#                 print(ndcg_score)\n",
    "\n",
    "            return np.mean(ndcgs)\n",
    "       \n",
    "    # idel\n",
    "    def _eval_test_set(self) -> float:\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            ndcgs = []\n",
    "            for cur_id in np.unique(self.query_ids_test):\n",
    "                mask = self.query_ids_test == cur_id\n",
    "                X_test_tmp = self.X_test[mask]\n",
    "                valid_pred = self.model(X_test_tmp)\n",
    "                ndcg_score = self._ndcg_k(\n",
    "                    self.ys_test[mask], valid_pred, self.ndcg_top_k)\n",
    "                if np.isnan(ndcg_score):\n",
    "                    ndcgs.append(0)\n",
    "                    continue\n",
    "                ndcgs.append(ndcg_score)\n",
    "            return np.mean(ndcgs)\n",
    "        \n",
    "    def _calc_loss(self, batch_ys: torch.FloatTensor,\n",
    "                   batch_pred: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \n",
    "        P_y_i = torch.softmax(batch_ys, dim=0) \n",
    "        P_z_i = torch.softmax(batch_pred, dim=0)\n",
    "        \n",
    "        loss = -torch.sum(P_y_i * torch.log(P_z_i))\n",
    "#         loss = -torch.sum(P_y_i * torch.log(P_z_i/P_y_i))\n",
    "        return loss\n",
    "\n",
    "        \n",
    "    def _ndcg_k(self, ys_true, ys_pred, ndcg_top_k) -> float:\n",
    "        def dcg(ys_true, ys_pred):\n",
    "            _, argsort = torch.sort(ys_pred, descending=True, dim=0)\n",
    "            argsort = argsort[:ndcg_top_k]\n",
    "            ys_true_sorted = ys_true[argsort]\n",
    "            ret = 0\n",
    "            for i, l in enumerate(ys_true_sorted, 1):\n",
    "                ret += (2 ** l - 1) / math.log2(1 + i)\n",
    "            return ret\n",
    "        ideal_dcg = dcg(ys_true, ys_true)\n",
    "        pred_dcg = dcg(ys_true, ys_pred)\n",
    "        return (pred_dcg / ideal_dcg).item()\n",
    "    \n",
    "    \n",
    "#     def _ndcg_k(self, ys_true: torch.Tensor, ys_pred: torch.Tensor,\n",
    "#                 ndcg_top_k: int) -> float:\n",
    "\n",
    "#         ys_ideal_true_sort, _ = torch.sort(ys_true, descending=True, dim=0)\n",
    "    \n",
    "#         if ndcg_top_k:\n",
    "#             ys_ideal_true_sort = ys_ideal_true_sort[:ndcg_top_k]\n",
    "\n",
    "#         ideal_dcg = 0\n",
    "#         for ix, e in enumerate(ys_ideal_true_sort):\n",
    "#             ideal_dcg += self.compute_gain(float(e), gain_scheme=\"exp2\")/math.log2(ix+2)\n",
    "\n",
    "#         dcg_ = self.dcg(ys_true, ys_pred, gain_scheme=\"exp2\", top_k=ndcg_top_k)\n",
    "        \n",
    "#         return dcg_/ideal_dcg\n",
    "\n",
    "\n",
    "#     def dcg(self, ys_true: torch.Tensor, ys_pred: torch.Tensor, gain_scheme: str, top_k: int) -> float:\n",
    "#         ys_pred_sort, sort_indices = torch.sort(ys_pred, descending=True, dim=0)\n",
    "#         ys_true_sort = ys_true[sort_indices]\n",
    "        \n",
    "#         if top_k:\n",
    "#             ys_true_sort = ys_true_sort[:top_k]\n",
    "\n",
    "#         dcg_ = 0\n",
    "#         for ix, e in enumerate(ys_true_sort):\n",
    "#             dcg_ += self.compute_gain(float(e), gain_scheme)/math.log2(ix+2)\n",
    "            \n",
    "#         return dcg_\n",
    "\n",
    "    \n",
    "#     def compute_gain(self, y_value: float, gain_scheme: str) -> float:\n",
    "#         if gain_scheme==\"const\":\n",
    "#             return y_value\n",
    "#         elif gain_scheme==\"exp2\":\n",
    "#             # return 2**y_value - 1\n",
    "#             return pow(2, y_value) - 1\n",
    "#         else:\n",
    "#             return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T13:45:17.300343Z",
     "start_time": "2021-07-12T13:45:15.034958Z"
    }
   },
   "outputs": [],
   "source": [
    "s = Solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T13:45:22.741443Z",
     "start_time": "2021-07-12T13:45:18.060152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "score on eval set: 0.4254\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "epoch 2\n",
      "score on eval set: 0.4214\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "epoch 3\n",
      "score on eval set: 0.4434\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "epoch 4\n",
      "score on eval set: 0.4248\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "epoch 5\n",
      "score on eval set: 0.4172\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = s.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T13:45:31.749676Z",
     "start_time": "2021-07-12T13:45:31.742943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42544628610717683,\n",
       " 0.4214029931984315,\n",
       " 0.443382901354363,\n",
       " 0.4247930105962125,\n",
       " 0.41723439299587833]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
